{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5215de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from persona_generation_prompt import PROMPT\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73339227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You must generate a set of fictional but realistic Iranian elderly personas that represent cultural, geographical, and social diversity in Iran.\n",
      "\n",
      "Rules:\n",
      "- All personas must be elderly (age 65–90).\n",
      "- Diversity must be reflected across all components, but keep the integrity and realism of each persona.\n",
      "- Personas should reflect cultural and social realities of Iran.\n",
      "- Reactions and attitudes do not need to be \"correct\" or \"moral\"; they may be shaped by culture, personal experience, or limitations.\n",
      "- Personality traits and psychological states should be consistent with the individual’s background.\n",
      "- Use the JSON format provided below.\n",
      "- For every variable, use values consistent with Iranian context.\n",
      "- Do not omit any variable.\n",
      "\n",
      "Variable definitions and accepted values:\n",
      "Here’s the translation of your variables and values into **English**:\n",
      "\n",
      "### 1. **Demographic Information**\n",
      "\n",
      "* **Age**: integer\n",
      "* **Gender**: [\"Male\", \"Female\", \"Non-binary\", \"Other\"]\n",
      "* **Marital Status**: [\"Single\", \"Married\", \"Widowed\", \"Divorced\", \"Separated\"]\n",
      "* **Children**: number or description (e.g., \"None\", \"1\", \"2-3\", \"4+\")\n",
      "* **Living Situation**: [\"Living with Family\", \"Living Alone\", \"Shared Housing\"]\n",
      "\n",
      "### 2. **Biological Component**\n",
      "\n",
      "* **General Health**: [\"Good\", \"Average\", \"Poor\"]\n",
      "* **Chronic Disease**: [None, \"High Blood Pressure\", \"Cardiovascular Diseases\", \"Type 2 Diabetes\", \"Arthritis and Joint Pain\", \"Osteoporosis\", \"Alzheimer's and Dementia\", \"Chronic Kidney Disease\", \"Chronic Obstructive Pulmonary Disease\", \"Chronic Depression and Anxiety\", \"Vision and Hearing Problems\", \"Chronic Liver Failure\", \"Parkinson's\", \"Chronic Sleep Disorders\", \"Chronic Gastrointestinal Issues\"]\n",
      "* **Mobility**: [\"Independent\", \"With Cane or Walker\", \"In Wheelchair\", \"Dependent\"]\n",
      "* **Hearing Senses**: [\"Good\", \"Average\", \"Poor\"]\n",
      "* **Vision Senses**: [\"Good\", \"Average\", \"Poor\"]\n",
      "* **Daily Energy**: [\"High\", \"Average\", \"Low\"]\n",
      "\n",
      "### 3. **Psychological Component**\n",
      "\n",
      "* **Personality Type**: [\"INTJ\", \"INTP\", \"ENTJ\", \"ENTP\", \"INFJ\", \"INFP\", \"ENFJ\", \"ENFP\", \"ISTJ\", \"ISFJ\", \"ESTJ\", \"ESFJ\", \"ISTP\", \"ISFP\", \"ESTP\", \"ESFP\"]\n",
      "* **Cognitive Status**: [\"Healthy Memory\", \"Mild Forgetfulness\", \"Alzheimer's\"]\n",
      "* **Dominant Emotion**: [\"Happy\", \"Sad\", \"Anxious\", \"Calm\"]\n",
      "* **Emotional Intelligence**: [\"Low\", \"Average\", \"High\"]\n",
      "* **IQ**: [\"Low\", \"Average\", \"High\"]\n",
      "* **Attitude Toward Aging**: [\"Acceptance\", \"Resistance\", \"Meaning-Seeking\", \"Denial\"]\n",
      "\n",
      "### 4. **Social Component**\n",
      "\n",
      "* **Main Social Role**: [\"Grandfather\", \"Grandmother\", \"Retired\", \"Social Activist\"]\n",
      "* **Social Support**: [\"Large Family\", \"Alone\", \"Supportive Friends\", \"Government Support\"]\n",
      "* **Social Participation**: [\"Active\", \"Inactive\"]\n",
      "\n",
      "### 5. **Economic Component**\n",
      "\n",
      "* **Income**: [\"Independent\", \"Retirement Pension\", \"Dependent on Children\", \"No Income\"]\n",
      "* **Economic Decile**: integer 1–10\n",
      "* **Housing**: [\"Own Home\", \"Rented\", \"Nursing Home\"]\n",
      "\n",
      "### 6. **Cultural-Value Component**\n",
      "\n",
      "* **Religion & Sect**: [\"Shia Muslim\", \"Sunni Muslim\", \"Christian\", \"Zoroastrian\", \"Jewish\"]\n",
      "* **Internalized Moral Traits**: list of 2–4 traits (positive or negative)\n",
      "* **Religiosity Level**: [\"Low\", \"Average\", \"High\"]\n",
      "* **Ethnicity**: [\"Persian\", \"Azeri\", \"Kurdish\", \"Lur\", \"Baloch\", \"Arab\", \"Turkmen\", \"Gilaki\", \"Mazandarani\", \"Qashqai\"]\n",
      "* **Language**: [\"Persian\", \"Azeri\", \"Kurdish\", \"Luri\", \"Balochi\", \"Arabic\", \"Turkmen\", \"Gilaki\", \"Mazandarani\", \"Qashqai\"]\n",
      "\n",
      "### 7. **Contextual Component**\n",
      "\n",
      "* **Important Personal Experiences**: [\"Immigration\", \"Career Success\", \"Loss of Loved Ones\", \"War Experience\", \"Economic Hardship\", \"Educational Achievement\", \"Battle with Serious Illness (e.g., Cancer, Chronic Disease)\"]\n",
      "* **Life Satisfaction**: [\"Satisfied\", \"Dissatisfied\", \"Neutral\"]\n",
      "* **Meaning and Purpose in Old Age**: [\"Helping Family\", \"Spiritual Activities\", \"Waiting for Death\", \"Pleasure-Seeking\"]\n",
      "\n",
      "\n",
      "JSON Format:\n",
      "[\n",
      "{\n",
      "  \"age\": \"\",\n",
      "  \"gender\": \"\",\n",
      "  \"marital_status\": \"\",\n",
      "  \"children\": \"\",\n",
      "  \"living_situation\": \"\",\n",
      "  \"general_health\": \"\",\n",
      "  \"chronic_disease\": \"\",\n",
      "  \"mobility\": \"\",\n",
      "  \"hearing_senses\": \"\",\n",
      "  \"vision_senses\": \"\",\n",
      "  \"daily_energy\": \"\",\n",
      "  \"personality_type\": \"\",\n",
      "  \"cognitive_status\": \"\",\n",
      "  \"dominant_emotion\": \"\",\n",
      "  \"emotional_intelligence\": \"\",\n",
      "  \"iq\": \"\",\n",
      "  \"attitude_toward_aging\": \"\",\n",
      "  \"main_social_role\": \"\",\n",
      "  \"social_support\": \"\",\n",
      "  \"social_participation\": \"\",\n",
      "  \"income\": \"\",\n",
      "  \"economic_decile\": \"\",\n",
      "  \"housing\": \"\",\n",
      "  \"religion_and_sect\": \"\",\n",
      "  \"internalized_moral_traits\": \"\",\n",
      "  \"religiosity_level\": \"\",\n",
      "  \"ethnicity\": \"\",\n",
      "  \"language\": \"\",\n",
      "  \"important_personal_experiences\": \"\",\n",
      "  \"life_satisfaction\": \"\",\n",
      "  \"meaning_and_purpose_in_old_age\": \"\"\n",
      "},\n",
      "...\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39b57b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "BASE_URL = \"https://api.metisai.ir/openai/v1\"\n",
    "API_KEY = os.getenv(\"METIS_API_KEY\")\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=API_KEY,\n",
    "    base_url=BASE_URL,\n",
    "    http_client=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "04619e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TO_USE = \"gpt-5-mini\"\n",
    "\n",
    "TEMPERATURE = 1\n",
    "TOP_P = 0.9\n",
    "PRESENCE_PENALTY = 0.3\n",
    "FREQUENCY_PENALTY = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "946f3b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages(persona_count: int):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Generate {persona_count} personas. Only give the JSON array with no extra text or formatting. Don't wrap the array in markdown formatting.\"\n",
    "        }\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ef683686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm_batch(model, messages, batch_count=1, batch_file_path=\"batch_input.jsonl\"):\n",
    "    \"\"\"\n",
    "    Call the OpenAI Batch API to process requests asynchronously.\n",
    "\n",
    "    Args:\n",
    "        model (str): The model to use.\n",
    "        messages (list): The list of messages to send (single prompt with system and user roles).\n",
    "        batch_count (int): The number of batches to create.\n",
    "        batch_file_path (str): Path to save the batch input file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Batch metadata including status and file IDs.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "    # Step 1: Prepare the batch input file\n",
    "    with open(batch_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i in range(batch_count):\n",
    "            batch_request = {\n",
    "                \"custom_id\": f\"request-{i+1}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": model,\n",
    "                    \"messages\": messages,\n",
    "                }\n",
    "            }\n",
    "            f.write(json.dumps(batch_request, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # Step 2: Upload the batch input file\n",
    "    batch_input_file = client.files.create(\n",
    "        file=open(batch_file_path, \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "\n",
    "    # Step 3: Create the batch\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=batch_input_file.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\n",
    "            \"description\": \"Batch processing for persona generation\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "48a9912d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1321\n",
      "{'single_persona_tokens': 278, 'total_personas_tokens': 13900}\n"
     ]
    }
   ],
   "source": [
    "# Token counting helpers using tiktoken\n",
    "# Requires: pip install tiktoken\n",
    "import tiktoken\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "def num_tokens_from_messages(messages: List[Dict[str, Any]], model: str = MODEL_TO_USE) -> int:\n",
    "    \"\"\"Return an estimate of the number of tokens used by a list of chat messages.\n",
    "\n",
    "    Uses heuristics commonly used with OpenAI chat models (tokens per message/name),\n",
    "    falling back to the `cl100k_base` encoding when the model encoding isn't available.\n",
    "\n",
    "    Note: exact token accounting depends on the model's internal tokenization; this\n",
    "    function gives a good practical estimate for budgeting and monitoring.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except Exception:\n",
    "        # fallback if model name is unknown to tiktoken\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    # Heuristics from public guidance; adjust if you know exact model rules\n",
    "    if model in (\"gpt-3.5-turbo-0301\", \"gpt-4-0314\"):\n",
    "        tokens_per_message = 4\n",
    "        tokens_per_name = -1\n",
    "    else:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "\n",
    "    total_tokens = 0\n",
    "    for message in messages:\n",
    "        total_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            # skip non-string values by converting to string\n",
    "            if not isinstance(value, str):\n",
    "                value = json.dumps(value, ensure_ascii=False)\n",
    "            total_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                total_tokens += tokens_per_name\n",
    "\n",
    "    total_tokens += 3  # assistant priming (heuristic)\n",
    "    return total_tokens\n",
    "\n",
    "\n",
    "def num_tokens_from_string(s: str, model: str = MODEL_TO_USE) -> int:\n",
    "    \"\"\"Return the number of tokens in a string for the given model's tokenizer.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except Exception:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(encoding.encode(s))\n",
    "\n",
    "\n",
    "SAMPLE_PERSONA = {\n",
    "    \"age\": 65,\n",
    "    \"gender\": \"Male\",\n",
    "    \"marital_status\": \"Married\",\n",
    "    \"children\": \"2-3\",\n",
    "    \"living_situation\": \"Living with Family\",\n",
    "    \"general_health\": \"Good\",\n",
    "    \"chronic_disease\": \"High Blood Pressure\",\n",
    "    \"mobility\": \"Independent\",\n",
    "    \"hearing_senses\": \"Good\",\n",
    "    \"vision_senses\": \"Good\",\n",
    "    \"daily_energy\": \"High\",\n",
    "    \"personality_type\": \"ISTJ\",\n",
    "    \"cognitive_status\": \"Healthy Memory\",\n",
    "    \"dominant_emotion\": \"Calm\",\n",
    "    \"emotional_intelligence\": \"High\",\n",
    "    \"iq\": \"Average\",\n",
    "    \"attitude_toward_aging\": \"Acceptance\",\n",
    "    \"main_social_role\": \"Grandfather\",\n",
    "    \"social_support\": \"Large Family\",\n",
    "    \"social_participation\": \"Active\",\n",
    "    \"income\": \"Retirement Pension\",\n",
    "    \"economic_decile\": 6,\n",
    "    \"housing\": \"Own Home\",\n",
    "    \"religion_and_sect\": \"Shia Muslim\",\n",
    "    \"internalized_moral_traits\": [\"Respectful\", \"Reliable\", \"Generous\"],\n",
    "    \"religiosity_level\": \"Average\",\n",
    "    \"ethnicity\": \"Persian\",\n",
    "    \"language\": \"Persian\",\n",
    "    \"important_personal_experiences\": \"Educational Achievement\",\n",
    "    \"life_satisfaction\": \"Satisfied\",\n",
    "    \"meaning_and_purpose_in_old_age\": \"Helping Family\"\n",
    "}\n",
    "\n",
    "\n",
    "def estimate_persona_tokens(persona_sample: Dict[str, Any], persona_count: int, model: str = MODEL_TO_USE) -> Dict[str, int]:\n",
    "    \"\"\"Estimate tokens for a single persona JSON and for persona_count copies of it.\n",
    "\n",
    "    Returns a dict with single_persona_tokens and total_personas_tokens.\n",
    "    \"\"\"\n",
    "    persona_str = json.dumps(persona_sample, ensure_ascii=False)\n",
    "    single = num_tokens_from_string(persona_str, model)\n",
    "    return {\"single_persona_tokens\": single, \"total_personas_tokens\": single * persona_count}\n",
    "\n",
    "\n",
    "def estimate_run_tokens(persona_count: int, response_text: str, model: str = MODEL_TO_USE) -> Dict[str, int]:\n",
    "    \"\"\"Estimate input/output tokens for a run that asks for persona_count personas.\n",
    "\n",
    "    - input_tokens: tokens consumed by the `messages(persona_count)` payload\n",
    "    - output_tokens: tokens in the LLM response text\n",
    "    - total: sum of input + output\n",
    "    \"\"\"\n",
    "    msgs = messages(persona_count)\n",
    "    input_tokens = num_tokens_from_messages(msgs, model)\n",
    "    output_tokens = num_tokens_from_string(response_text or \"\", model)\n",
    "    return {\"input_tokens\": input_tokens, \"output_tokens\": output_tokens, \"total\": input_tokens + output_tokens}\n",
    "\n",
    "\n",
    "# Usage examples (uncomment to run):\n",
    "print(num_tokens_from_messages(messages(1)))\n",
    "print(estimate_persona_tokens(SAMPLE_PERSONA, 50))\n",
    "# if 'personas' in globals():\n",
    "#     print(estimate_run_tokens(50, personas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f7dfc7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types import Batch\n",
    "\n",
    "def poll_batch_status(batch: Batch):\n",
    "    batch_id = batch.id\n",
    "\n",
    "    resp = client.batches.retrieve(batch_id)\n",
    "\n",
    "    if resp.status == \"completed\":\n",
    "        if resp.output_file_id:\n",
    "            file_response = client.files.content(resp.output_file_id)\n",
    "            return file_response\n",
    "        elif resp.error_file_id:\n",
    "            file_response = client.files.content(resp.error_file_id)\n",
    "            return file_response\n",
    "        else:\n",
    "            print(\"Batch completed but no output_file_id\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Batch status: {resp.status}. Not completed yet.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "634fc914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(resp):\n",
    "    contents = []\n",
    "    if resp:\n",
    "        answers = resp.text.split(\"\\n\")[:-1]\n",
    "\n",
    "        for batch_answer in answers:\n",
    "            single_batch_resp = json.loads(batch_answer)\n",
    "\n",
    "            answer = single_batch_resp['response']['body']['choices'][0]['message']['content']\n",
    "            contents.append(answer)\n",
    "\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07b3b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def save_batch_output(batch):\n",
    "    timestamp = time()\n",
    "\n",
    "    output_path = f\"personas/batch_output_{timestamp}.jsonl\"\n",
    "    result = poll_batch_status(batch)\n",
    "    if not result:\n",
    "        print(\"Batch not completed yet or no output available.\")\n",
    "        return\n",
    "        \n",
    "    parsed = parse_response(result)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for p in parsed:\n",
    "            f.write(json.dumps(json.loads(p)) + \"\\n\")\n",
    "\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "879390a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the capital of France?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "resp = call_llm_batch(MODEL_TO_USE, test_messages, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84787aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The capital of France is Paris.', 'The capital of France is Paris.']\n"
     ]
    }
   ],
   "source": [
    "result = poll_batch_status(resp)\n",
    "if result:\n",
    "    print(parse_response(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d4dc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = call_llm_batch(MODEL_TO_USE, messages(10), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18619725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openai._legacy_response.HttpxBinaryResponseContent at 0x7358f65638c0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = poll_batch_status(resp)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9543521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_batch_output(resp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
