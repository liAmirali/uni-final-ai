{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bd178e9",
   "metadata": {},
   "source": [
    "# Persona Analysis\n",
    "### Comprehensive analysis of all persona variables with distributions, charts, and statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "439c962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d786cc",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43992038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 5 personas from personas/personas1759869184.9239116.json\n",
      "‚úì Loaded 5 personas from personas/personas1759869218.5306675.json\n",
      "‚úì Loaded 10 personas from personas/personas1759869291.3396184.json\n",
      "‚úì Loaded 10 personas from personas/personas1759869971.4475083.json\n",
      "\n",
      "üìä Total personas loaded: 30\n",
      "\n",
      "üìÅ File breakdown:\n",
      "  - personas1759869184.9239116.json: 5 personas\n",
      "  - personas1759869218.5306675.json: 5 personas\n",
      "  - personas1759869291.3396184.json: 10 personas\n",
      "  - personas1759869971.4475083.json: 10 personas\n"
     ]
    }
   ],
   "source": [
    "# List of JSON file paths to analyze\n",
    "json_files = [\n",
    "    'personas/personas1759869184.9239116.json',\n",
    "    'personas/personas1759869218.5306675.json',\n",
    "    'personas/personas1759869291.3396184.json',\n",
    "    'personas/personas1759869971.4475083.json',\n",
    "]\n",
    "\n",
    "# Load all personas\n",
    "all_personas = []\n",
    "file_stats = {}\n",
    "\n",
    "for file_path in json_files:\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            file_stats[file_path] = len(data)\n",
    "            all_personas.extend(data)\n",
    "            print(f\"‚úì Loaded {len(data)} personas from {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error loading {file_path}: {e}\")\n",
    "\n",
    "print(f\"\\nüìä Total personas loaded: {len(all_personas)}\")\n",
    "print(f\"\\nüìÅ File breakdown:\")\n",
    "for file, count in file_stats.items():\n",
    "    print(f\"  - {Path(file).name}: {count} personas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0fa5eb",
   "metadata": {},
   "source": [
    "## 2. Flatten and Structure Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "130aaa0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä DataFrame shape: 30 rows √ó 31 columns\n",
      "\n",
      "üîç DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30 entries, 0 to 29\n",
      "Data columns (total 31 columns):\n",
      " #   Column                        Non-Null Count  Dtype \n",
      "---  ------                        --------------  ----- \n",
      " 0   id                            30 non-null     int64 \n",
      " 1   age                           30 non-null     int64 \n",
      " 2   gender                        30 non-null     object\n",
      " 3   bio_general_health            30 non-null     object\n",
      " 4   bio_chronic_disease           24 non-null     object\n",
      " 5   bio_mobility                  30 non-null     object\n",
      " 6   bio_vision                    30 non-null     object\n",
      " 7   bio_hearing                   30 non-null     object\n",
      " 8   bio_daily_energy              30 non-null     object\n",
      " 9   psych_personality_type        30 non-null     object\n",
      " 10  psych_cognitive_status        30 non-null     object\n",
      " 11  psych_dominant_emotion        30 non-null     object\n",
      " 12  psych_emotional_intelligence  30 non-null     object\n",
      " 13  psych_iq                      30 non-null     object\n",
      " 14  psych_attitude_to_aging       30 non-null     object\n",
      " 15  social_main_role              30 non-null     object\n",
      " 16  social_support                30 non-null     object\n",
      " 17  social_participation          30 non-null     object\n",
      " 18  econ_income                   30 non-null     object\n",
      " 19  econ_decile                   30 non-null     int64 \n",
      " 20  econ_housing                  30 non-null     object\n",
      " 21  cultural_religion             30 non-null     object\n",
      " 22  cultural_moral_traits         30 non-null     object\n",
      " 23  cultural_religiosity_level    30 non-null     object\n",
      " 24  cultural_ethnicity            30 non-null     object\n",
      " 25  cultural_language             30 non-null     object\n",
      " 26  cultural_nationality          30 non-null     object\n",
      " 27  context_personal_experiences  30 non-null     object\n",
      " 28  context_historical_events     30 non-null     object\n",
      " 29  context_life_satisfaction     30 non-null     object\n",
      " 30  context_meaning_purpose       30 non-null     object\n",
      "dtypes: int64(3), object(28)\n",
      "memory usage: 7.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Flatten nested JSON structure into DataFrame\n",
    "def flatten_persona(persona):\n",
    "    flat = {\n",
    "        'id': persona.get('id'),\n",
    "        'age': persona.get('age'),\n",
    "        'gender': persona.get('gender'),\n",
    "    }\n",
    "    \n",
    "    # Biological component\n",
    "    bio = persona.get('biological_component', {})\n",
    "    flat['bio_general_health'] = bio.get('general_health')\n",
    "    flat['bio_chronic_disease'] = bio.get('chronic_disease')\n",
    "    flat['bio_mobility'] = bio.get('mobility')\n",
    "    senses = bio.get('senses', {})\n",
    "    flat['bio_vision'] = senses.get('ÿ®€åŸÜÿß€å€å')\n",
    "    flat['bio_hearing'] = senses.get('ÿ¥ŸÜŸàÿß€å€å')\n",
    "    flat['bio_daily_energy'] = bio.get('daily_energy')\n",
    "    \n",
    "    # Psychological component\n",
    "    psych = persona.get('psychological_component', {})\n",
    "    flat['psych_personality_type'] = psych.get('personality_type')\n",
    "    flat['psych_cognitive_status'] = psych.get('cognitive_status')\n",
    "    flat['psych_dominant_emotion'] = psych.get('dominant_emotion')\n",
    "    flat['psych_emotional_intelligence'] = psych.get('emotional_intelligence')\n",
    "    flat['psych_iq'] = psych.get('iq')\n",
    "    flat['psych_attitude_to_aging'] = psych.get('attitude_to_aging')\n",
    "    \n",
    "    # Social component\n",
    "    social = persona.get('social_component', {})\n",
    "    flat['social_main_role'] = social.get('main_social_role')\n",
    "    flat['social_support'] = social.get('social_support')\n",
    "    flat['social_participation'] = social.get('social_participation')\n",
    "    \n",
    "    # Economic component\n",
    "    econ = persona.get('economic_component', {})\n",
    "    flat['econ_income'] = econ.get('income')\n",
    "    flat['econ_decile'] = econ.get('economic_decile')\n",
    "    flat['econ_housing'] = econ.get('housing')\n",
    "    \n",
    "    # Cultural value component\n",
    "    cultural = persona.get('cultural_value_component', {})\n",
    "    flat['cultural_religion'] = cultural.get('religion')\n",
    "    flat['cultural_moral_traits'] = cultural.get('moral_traits', [])\n",
    "    flat['cultural_religiosity_level'] = cultural.get('religiosity_level')\n",
    "    cultural_id = cultural.get('cultural_identity', {})\n",
    "    flat['cultural_ethnicity'] = cultural_id.get('ethnicity')\n",
    "    flat['cultural_language'] = cultural_id.get('language')\n",
    "    flat['cultural_nationality'] = cultural_id.get('nationality')\n",
    "    \n",
    "    # Contextual component\n",
    "    context = persona.get('contextual_component', {})\n",
    "    flat['context_personal_experiences'] = context.get('personal_experiences', [])\n",
    "    flat['context_historical_events'] = context.get('historical_events', [])\n",
    "    flat['context_life_satisfaction'] = context.get('life_satisfaction')\n",
    "    flat['context_meaning_purpose'] = context.get('meaning_and_purpose')\n",
    "    \n",
    "    return flat\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame([flatten_persona(p) for p in all_personas])\n",
    "\n",
    "print(f\"üìä DataFrame shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"\\nüîç DataFrame Info:\")\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c8cb8a",
   "metadata": {},
   "source": [
    "## 3. Basic Statistics Overview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0499ea91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "SAMPLE DATA (First 3 Personas)\n",
      "====================================================================================================\n",
      "   id  age gender bio_general_health      bio_chronic_disease    bio_mobility bio_vision bio_hearing bio_daily_energy psych_personality_type psych_cognitive_status psych_dominant_emotion psych_emotional_intelligence psych_iq psych_attitude_to_aging social_main_role   social_support social_participation     econ_income  econ_decile   econ_housing cultural_religion            cultural_moral_traits cultural_religiosity_level cultural_ethnicity cultural_language cultural_nationality                  context_personal_experiences context_historical_events context_life_satisfaction context_meaning_purpose\n",
      "0   1   72      F              ŸÖÿ™Ÿàÿ≥ÿ∑              ÿØ€åÿßÿ®ÿ™ ŸÜŸàÿπ €≤           ŸÖÿ≥ÿ™ŸÇŸÑ       ÿ∂ÿπ€åŸÅ         ÿÆŸàÿ®            ŸÖÿ™Ÿàÿ≥ÿ∑                   ISFJ           ŸÅÿ±ÿßŸÖŸàÿ¥€å ÿÆŸÅ€åŸÅ                   ÿ¢ÿ±ÿßŸÖ                         ÿ≤€åÿßÿØ    ŸÖÿ™Ÿàÿ≥ÿ∑                   Ÿæÿ∞€åÿ±ÿ¥         ŸÖÿßÿØÿ±ÿ®ÿ≤ÿ±⁄Ø  ÿÆÿßŸÜŸàÿßÿØŸá Ÿæÿ±ÿ¨ŸÖÿπ€åÿ™             ÿ∫€åÿ± ŸÅÿπÿßŸÑ  ÿ≠ŸÇŸàŸÇ ÿ®ÿßÿ≤ŸÜÿ¥ÿ≥ÿ™⁄Ø€å            4      ÿÆÿßŸÜŸá ÿ¥ÿÆÿµ€å       ŸÖÿ≥ŸÑŸÖÿßŸÜ ÿ¥€åÿπŸá           [ÿµÿ®Ÿàÿ±€å, ŸÖŸáÿ±ÿ®ÿßŸÜ€å, ⁄Øÿ∞ÿ¥ÿ™]                       ÿ≤€åÿßÿØ               ŸÅÿßÿ±ÿ≥             ŸÅÿßÿ±ÿ≥€å               ÿß€åÿ±ÿßŸÜ€å  [ÿßÿ≤ ÿØÿ≥ÿ™ ÿØÿßÿØŸÜ ŸáŸÖÿ≥ÿ±, ÿ≠ÿ∂Ÿàÿ± ŸÅÿπÿßŸÑ ÿØÿ± ŸÖÿ±ÿßÿ≥ŸÖ ŸÖÿ∞Ÿáÿ®€å]             [ÿßŸÜŸÇŸÑÿßÿ®, ÿ¨ŸÜ⁄Ø]                      ÿ±ÿßÿ∂€å          ⁄©ŸÖ⁄© ÿ®Ÿá ÿÆÿßŸÜŸàÿßÿØŸá\n",
      "1   2   80      M               ÿ∂ÿπ€åŸÅ  ÿ®€åŸÖÿßÿ±€å‚ÄåŸáÿß€å ŸÇŸÑÿ®€å Ÿà ÿπÿ±ŸàŸÇ€å  ÿ®ÿß ÿπÿµÿß €åÿß Ÿàÿß⁄©ÿ±       ÿ∂ÿπ€åŸÅ        ÿ∂ÿπ€åŸÅ               ⁄©ŸÖ                   ESTJ             ÿ≠ÿßŸÅÿ∏Ÿá ÿ≥ÿßŸÑŸÖ                  ŸÖÿ∂ÿ∑ÿ±ÿ®                           ⁄©ŸÖ    ŸÖÿ™Ÿàÿ≥ÿ∑                  ŸÖŸÇÿßŸàŸÖÿ™          ŸæÿØÿ±ÿ®ÿ≤ÿ±⁄Ø      ÿ≠ŸÖÿß€åÿ™ ÿØŸàŸÑÿ™€å             ÿ∫€åÿ± ŸÅÿπÿßŸÑ      ŸÅÿßŸÇÿØ ÿØÿ±ÿ¢ŸÖÿØ            2  ÿÆÿßŸÜŸá ÿ≥ÿßŸÑŸÖŸÜÿØÿßŸÜ        ŸÖÿ≥ŸÑŸÖÿßŸÜ ÿ≥ŸÜ€å                  [ÿ¨ÿØ€åÿ™, ÿ®ÿØ⁄ØŸÖÿßŸÜ€å]                      ŸÖÿ™Ÿàÿ≥ÿ∑               ÿ®ŸÑŸà⁄Ü             ÿ®ŸÑŸà⁄Ü€å               ÿß€åÿ±ÿßŸÜ€å  [ŸÖŸáÿßÿ¨ÿ±ÿ™ ÿßÿ≤ ÿ±Ÿàÿ≥ÿ™ÿß ÿ®Ÿá ÿ¥Ÿáÿ±, ÿßÿ≤ ÿØÿ≥ÿ™ ÿØÿßÿØŸÜ ÿπÿ≤€åÿ≤ÿßŸÜ]           [ÿßŸÜŸÇŸÑÿßÿ®, ⁄©ÿ±ŸàŸÜÿß]                    ŸÜÿßÿ±ÿßÿ∂€å              ÿßŸÜÿ™ÿ∏ÿßÿ± ŸÖÿ±⁄Ø\n",
      "2   3   69      F                ÿÆŸàÿ®       ÿ¢ÿ±ÿ™ÿ±Ÿàÿ≤ Ÿà ÿØÿ±ÿØ ŸÖŸÅÿßÿµŸÑ           ŸÖÿ≥ÿ™ŸÇŸÑ        ÿÆŸàÿ®         ÿÆŸàÿ®             ÿ®ÿßŸÑÿß                   ENFP             ÿ≠ÿßŸÅÿ∏Ÿá ÿ≥ÿßŸÑŸÖ                    ÿ¥ÿßÿØ                         ÿ≤€åÿßÿØ     ÿ≤€åÿßÿØ                ŸÖÿπŸÜÿßÿ¨Ÿà€å€å     ŸÅÿπÿßŸÑ ÿßÿ¨ÿ™ŸÖÿßÿπ€å      ÿØŸàÿ≥ÿ™ÿßŸÜ ÿ≠ÿßŸÖ€å                 ŸÅÿπÿßŸÑ           ŸÖÿ≥ÿ™ŸÇŸÑ            7      ÿÆÿßŸÜŸá ÿ¥ÿÆÿµ€å            ÿ≤ÿ±ÿ™ÿ¥ÿ™€å  [ŸÖÿ´ÿ®ÿ™‚ÄåÿßŸÜÿØ€åÿ¥€å, ÿ¥ÿ¨ÿßÿπÿ™, ŸÜŸàÿπ‚ÄåÿØŸàÿ≥ÿ™€å]                         ⁄©ŸÖ             ÿ≤ÿ±ÿ™ÿ¥ÿ™€å             ŸÅÿßÿ±ÿ≥€å               ÿß€åÿ±ÿßŸÜ€å        [ŸÅÿπÿßŸÑ€åÿ™ ŸÖÿØŸÜ€å ÿ®ÿ±ÿß€å ÿ≤ŸÜÿßŸÜ, ÿ≥ŸÅÿ±Ÿáÿß€å ŸÅÿ±ŸáŸÜ⁄Ø€å]                  [ÿßŸÜŸÇŸÑÿßÿ®]                      ÿ±ÿßÿ∂€å            ŸÅÿπÿßŸÑ€åÿ™ ŸÖÿπŸÜŸà€å\n",
      "\n",
      "====================================================================================================\n",
      "BASIC STATISTICS\n",
      "====================================================================================================\n",
      "Total number of personas: 30\n",
      "Number of features: 31\n",
      "\n",
      "Missing values per column:\n",
      "bio_chronic_disease    6\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "id                               int64\n",
      "age                              int64\n",
      "gender                          object\n",
      "bio_general_health              object\n",
      "bio_chronic_disease             object\n",
      "bio_mobility                    object\n",
      "bio_vision                      object\n",
      "bio_hearing                     object\n",
      "bio_daily_energy                object\n",
      "psych_personality_type          object\n",
      "psych_cognitive_status          object\n",
      "psych_dominant_emotion          object\n",
      "psych_emotional_intelligence    object\n",
      "psych_iq                        object\n",
      "psych_attitude_to_aging         object\n",
      "social_main_role                object\n",
      "social_support                  object\n",
      "social_participation            object\n",
      "econ_income                     object\n",
      "econ_decile                      int64\n",
      "econ_housing                    object\n",
      "cultural_religion               object\n",
      "cultural_moral_traits           object\n",
      "cultural_religiosity_level      object\n",
      "cultural_ethnicity              object\n",
      "cultural_language               object\n",
      "cultural_nationality            object\n",
      "context_personal_experiences    object\n",
      "context_historical_events       object\n",
      "context_life_satisfaction       object\n",
      "context_meaning_purpose         object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Display first few rows\n",
    "print(\"=\" * 100)\n",
    "print(\"SAMPLE DATA (First 3 Personas)\")\n",
    "print(\"=\" * 100)\n",
    "print(df.head(3).to_string())\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"BASIC STATISTICS\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"Total number of personas: {len(df)}\")\n",
    "print(f\"Number of features: {df.shape[1]}\")\n",
    "print(f\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "\n",
    "# Data types\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2f3b23",
   "metadata": {},
   "source": [
    "## 4. Demographic Analysis (Age & Gender)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9905a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGE ANALYSIS\n",
    "print(\"=\" * 100)\n",
    "print(\"AGE DISTRIBUTION\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\nAge statistics:\")\n",
    "print(df['age'].describe())\n",
    "print(f\"\\nAge range: {df['age'].min()} - {df['age'].max()}\")\n",
    "print(f\"Most common age: {df['age'].mode()[0] if not df['age'].mode().empty else 'N/A'}\")\n",
    "\n",
    "# Age distribution table\n",
    "age_counts = df['age'].value_counts().sort_index()\n",
    "print(f\"\\nAge frequency table:\")\n",
    "print(age_counts.to_string())\n",
    "\n",
    "# GENDER ANALYSIS\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"GENDER DISTRIBUTION\")\n",
    "print(\"=\" * 100)\n",
    "gender_counts = df['gender'].value_counts()\n",
    "gender_pct = df['gender'].value_counts(normalize=True) * 100\n",
    "print(f\"\\nGender counts:\")\n",
    "for gender, count in gender_counts.items():\n",
    "    print(f\"  {gender}: {count} ({gender_pct[gender]:.2f}%)\")\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Age histogram\n",
    "axes[0].hist(df['age'], bins=15, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Age', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Age Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Age boxplot\n",
    "axes[1].boxplot(df['age'], vert=True)\n",
    "axes[1].set_ylabel('Age', fontsize=12)\n",
    "axes[1].set_title('Age Boxplot', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Gender pie chart\n",
    "colors = sns.color_palette('husl', len(gender_counts))\n",
    "axes[2].pie(gender_counts, labels=gender_counts.index, autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "axes[2].set_title('Gender Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Age by Gender\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"AGE BY GENDER\")\n",
    "print(\"=\" * 100)\n",
    "age_by_gender = df.groupby('gender')['age'].describe()\n",
    "print(age_by_gender.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe41215",
   "metadata": {},
   "source": [
    "## 5. Biological Component Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0052f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biological component variables\n",
    "bio_vars = ['bio_general_health', 'bio_chronic_disease', 'bio_mobility', \n",
    "            'bio_vision', 'bio_hearing', 'bio_daily_energy']\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"BIOLOGICAL COMPONENT - COMPLETE ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Analyze each biological variable\n",
    "for var in bio_vars:\n",
    "    print(f\"\\n{'=' * 100}\")\n",
    "    print(f\"{var.upper().replace('BIO_', '')}\")\n",
    "    print(f\"{'=' * 100}\")\n",
    "    \n",
    "    # Clean data: remove None/NaN values\n",
    "    clean_data = df[var].dropna()\n",
    "    \n",
    "    print(f\"Total responses: {len(clean_data)} (Missing: {df[var].isnull().sum()})\")\n",
    "    print(f\"\\nValue counts:\")\n",
    "    value_counts = clean_data.value_counts()\n",
    "    value_pct = clean_data.value_counts(normalize=True) * 100\n",
    "    \n",
    "    for value, count in value_counts.items():\n",
    "        print(f\"  {value}: {count} ({value_pct[value]:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nUnique values: {clean_data.nunique()}\")\n",
    "    print(f\"Available values: {sorted(clean_data.unique().tolist())}\")\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, var in enumerate(bio_vars):\n",
    "    clean_data = df[var].dropna()\n",
    "    value_counts = clean_data.value_counts()\n",
    "    \n",
    "    # Bar chart\n",
    "    axes[idx].bar(range(len(value_counts)), value_counts.values, \n",
    "                  color=sns.color_palette('husl', len(value_counts)))\n",
    "    axes[idx].set_xticks(range(len(value_counts)))\n",
    "    axes[idx].set_xticklabels(value_counts.index, rotation=45, ha='right', fontsize=9)\n",
    "    axes[idx].set_ylabel('Frequency', fontsize=10)\n",
    "    axes[idx].set_title(var.replace('bio_', '').replace('_', ' ').title(), \n",
    "                        fontsize=12, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(value_counts.values):\n",
    "        axes[idx].text(i, v, str(v), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cross-tabulation: General Health vs Daily Energy\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"CROSS-TABULATION: General Health vs Daily Energy\")\n",
    "print(\"=\" * 100)\n",
    "crosstab = pd.crosstab(df['bio_general_health'], df['bio_daily_energy'], margins=True)\n",
    "print(crosstab.to_string())\n",
    "\n",
    "# Mobility vs General Health\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"CROSS-TABULATION: Mobility vs General Health\")\n",
    "print(\"=\" * 100)\n",
    "crosstab2 = pd.crosstab(df['bio_mobility'], df['bio_general_health'], margins=True)\n",
    "print(crosstab2.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36a5231",
   "metadata": {},
   "source": [
    "## 6. Psychological Component Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe775f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Psychological component variables\n",
    "psych_vars = ['psych_personality_type', 'psych_cognitive_status', 'psych_dominant_emotion',\n",
    "              'psych_emotional_intelligence', 'psych_iq', 'psych_attitude_to_aging']\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"PSYCHOLOGICAL COMPONENT - COMPLETE ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Analyze each psychological variable\n",
    "for var in psych_vars:\n",
    "    print(f\"\\n{'=' * 100}\")\n",
    "    print(f\"{var.upper().replace('PSYCH_', '')}\")\n",
    "    print(f\"{'=' * 100}\")\n",
    "    \n",
    "    # Clean data: remove None/NaN values\n",
    "    clean_data = df[var].dropna()\n",
    "    \n",
    "    print(f\"Total responses: {len(clean_data)} (Missing: {df[var].isnull().sum()})\")\n",
    "    print(f\"\\nValue counts:\")\n",
    "    value_counts = clean_data.value_counts()\n",
    "    value_pct = clean_data.value_counts(normalize=True) * 100\n",
    "    \n",
    "    for value, count in value_counts.items():\n",
    "        print(f\"  {value}: {count} ({value_pct[value]:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nUnique values: {clean_data.nunique()}\")\n",
    "    print(f\"Available values: {sorted(clean_data.unique().tolist())}\")\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, var in enumerate(psych_vars):\n",
    "    clean_data = df[var].dropna()\n",
    "    value_counts = clean_data.value_counts()\n",
    "    \n",
    "    # Bar chart\n",
    "    axes[idx].bar(range(len(value_counts)), value_counts.values, \n",
    "                  color=sns.color_palette('Set2', len(value_counts)))\n",
    "    axes[idx].set_xticks(range(len(value_counts)))\n",
    "    axes[idx].set_xticklabels(value_counts.index, rotation=45, ha='right', fontsize=9)\n",
    "    axes[idx].set_ylabel('Frequency', fontsize=10)\n",
    "    axes[idx].set_title(var.replace('psych_', '').replace('_', ' ').title(), \n",
    "                        fontsize=12, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(value_counts.values):\n",
    "        axes[idx].text(i, v, str(v), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# MBTI Personality Type Analysis\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"MBTI PERSONALITY TYPE BREAKDOWN\")\n",
    "print(\"=\" * 100)\n",
    "mbti_data = df['psych_personality_type'].dropna()\n",
    "print(f\"Total MBTI entries: {len(mbti_data)}\")\n",
    "print(f\"\\nMBTI Distribution:\")\n",
    "print(mbti_data.value_counts().to_string())\n",
    "\n",
    "# Analyze MBTI dimensions\n",
    "if len(mbti_data) > 0:\n",
    "    print(f\"\\nMBTI Dimension Analysis:\")\n",
    "    mbti_list = mbti_data.tolist()\n",
    "    \n",
    "    # Extract dimensions\n",
    "    e_i = [p[0] for p in mbti_list if len(p) == 4]\n",
    "    s_n = [p[1] for p in mbti_list if len(p) == 4]\n",
    "    t_f = [p[2] for p in mbti_list if len(p) == 4]\n",
    "    j_p = [p[3] for p in mbti_list if len(p) == 4]\n",
    "    \n",
    "    print(f\"  E vs I: E={e_i.count('E')}, I={e_i.count('I')}\")\n",
    "    print(f\"  S vs N: S={s_n.count('S')}, N={s_n.count('N')}\")\n",
    "    print(f\"  T vs F: T={t_f.count('T')}, F={t_f.count('F')}\")\n",
    "    print(f\"  J vs P: J={j_p.count('J')}, P={j_p.count('P')}\")\n",
    "\n",
    "# Cognitive Status vs Dominant Emotion\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"CROSS-TABULATION: Cognitive Status vs Dominant Emotion\")\n",
    "print(\"=\" * 100)\n",
    "crosstab = pd.crosstab(df['psych_cognitive_status'], df['psych_dominant_emotion'], margins=True)\n",
    "print(crosstab.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ebe9e1",
   "metadata": {},
   "source": [
    "## 7. Social Component Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee4cc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Social component variables\n",
    "social_vars = ['social_main_role', 'social_support', 'social_participation']\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"SOCIAL COMPONENT - COMPLETE ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Analyze each social variable\n",
    "for var in social_vars:\n",
    "    print(f\"\\n{'=' * 100}\")\n",
    "    print(f\"{var.upper().replace('SOCIAL_', '')}\")\n",
    "    print(f\"{'=' * 100}\")\n",
    "    \n",
    "    # Clean data: remove None/NaN values\n",
    "    clean_data = df[var].dropna()\n",
    "    \n",
    "    print(f\"Total responses: {len(clean_data)} (Missing: {df[var].isnull().sum()})\")\n",
    "    print(f\"\\nValue counts:\")\n",
    "    value_counts = clean_data.value_counts()\n",
    "    value_pct = clean_data.value_counts(normalize=True) * 100\n",
    "    \n",
    "    for value, count in value_counts.items():\n",
    "        print(f\"  {value}: {count} ({value_pct[value]:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nUnique values: {clean_data.nunique()}\")\n",
    "    print(f\"Available values: {sorted(clean_data.unique().tolist())}\")\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "for idx, var in enumerate(social_vars):\n",
    "    clean_data = df[var].dropna()\n",
    "    value_counts = clean_data.value_counts()\n",
    "    \n",
    "    # Bar chart\n",
    "    axes[idx].bar(range(len(value_counts)), value_counts.values, \n",
    "                  color=sns.color_palette('Pastel1', len(value_counts)))\n",
    "    axes[idx].set_xticks(range(len(value_counts)))\n",
    "    axes[idx].set_xticklabels(value_counts.index, rotation=45, ha='right', fontsize=9)\n",
    "    axes[idx].set_ylabel('Frequency', fontsize=10)\n",
    "    axes[idx].set_title(var.replace('social_', '').replace('_', ' ').title(), \n",
    "                        fontsize=12, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(value_counts.values):\n",
    "        axes[idx].text(i, v, str(v), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cross-tabulation: Social Support vs Social Participation\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"CROSS-TABULATION: Social Support vs Social Participation\")\n",
    "print(\"=\" * 100)\n",
    "crosstab = pd.crosstab(df['social_support'], df['social_participation'], margins=True)\n",
    "print(crosstab.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ce29cf",
   "metadata": {},
   "source": [
    "## 8. Economic Component Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaa8dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Economic component variables\n",
    "econ_vars = ['econ_income', 'econ_decile', 'econ_housing']\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"ECONOMIC COMPONENT - COMPLETE ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Analyze each economic variable\n",
    "for var in econ_vars:\n",
    "    print(f\"\\n{'=' * 100}\")\n",
    "    print(f\"{var.upper().replace('ECON_', '')}\")\n",
    "    print(f\"{'=' * 100}\")\n",
    "    \n",
    "    # Clean data: remove None/NaN values\n",
    "    clean_data = df[var].dropna()\n",
    "    \n",
    "    print(f\"Total responses: {len(clean_data)} (Missing: {df[var].isnull().sum()})\")\n",
    "    \n",
    "    # For numeric data (economic_decile)\n",
    "    if var == 'econ_decile':\n",
    "        print(f\"\\nStatistics:\")\n",
    "        print(clean_data.describe())\n",
    "        print(f\"\\nValue counts:\")\n",
    "        value_counts = clean_data.value_counts().sort_index()\n",
    "    else:\n",
    "        print(f\"\\nValue counts:\")\n",
    "        value_counts = clean_data.value_counts()\n",
    "    \n",
    "    value_pct = value_counts / len(clean_data) * 100\n",
    "    \n",
    "    for value, count in value_counts.items():\n",
    "        print(f\"  {value}: {count} ({value_pct[value]:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nUnique values: {clean_data.nunique()}\")\n",
    "    if var != 'econ_decile':\n",
    "        print(f\"Available values: {sorted(clean_data.unique().tolist())}\")\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# Income\n",
    "clean_data = df['econ_income'].dropna()\n",
    "value_counts = clean_data.value_counts()\n",
    "axes[0].bar(range(len(value_counts)), value_counts.values, \n",
    "            color=sns.color_palette('Set3', len(value_counts)))\n",
    "axes[0].set_xticks(range(len(value_counts)))\n",
    "axes[0].set_xticklabels(value_counts.index, rotation=45, ha='right', fontsize=9)\n",
    "axes[0].set_ylabel('Frequency', fontsize=10)\n",
    "axes[0].set_title('Income Source', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(value_counts.values):\n",
    "    axes[0].text(i, v, str(v), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Economic Decile\n",
    "clean_data = df['econ_decile'].dropna()\n",
    "value_counts = clean_data.value_counts().sort_index()\n",
    "axes[1].bar(value_counts.index, value_counts.values, \n",
    "            color=sns.color_palette('viridis', len(value_counts)))\n",
    "axes[1].set_xlabel('Economic Decile', fontsize=10)\n",
    "axes[1].set_ylabel('Frequency', fontsize=10)\n",
    "axes[1].set_title('Economic Decile Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in zip(value_counts.index, value_counts.values):\n",
    "    axes[1].text(i, v, str(v), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Housing\n",
    "clean_data = df['econ_housing'].dropna()\n",
    "value_counts = clean_data.value_counts()\n",
    "axes[2].bar(range(len(value_counts)), value_counts.values, \n",
    "            color=sns.color_palette('Set3', len(value_counts)))\n",
    "axes[2].set_xticks(range(len(value_counts)))\n",
    "axes[2].set_xticklabels(value_counts.index, rotation=45, ha='right', fontsize=9)\n",
    "axes[2].set_ylabel('Frequency', fontsize=10)\n",
    "axes[2].set_title('Housing Type', fontsize=12, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(value_counts.values):\n",
    "    axes[2].text(i, v, str(v), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cross-tabulation: Income vs Housing\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"CROSS-TABULATION: Income vs Housing\")\n",
    "print(\"=\" * 100)\n",
    "crosstab = pd.crosstab(df['econ_income'], df['econ_housing'], margins=True)\n",
    "print(crosstab.to_string())\n",
    "\n",
    "# Economic Decile statistics by Income\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ECONOMIC DECILE BY INCOME SOURCE\")\n",
    "print(\"=\" * 100)\n",
    "decile_by_income = df.groupby('econ_income')['econ_decile'].describe()\n",
    "print(decile_by_income.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed14708a",
   "metadata": {},
   "source": [
    "## 9. Cultural & Value Component Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24acebd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cultural component variables (excluding moral_traits which is a list)\n",
    "cultural_vars = ['cultural_religion', 'cultural_religiosity_level', \n",
    "                 'cultural_ethnicity', 'cultural_language', 'cultural_nationality']\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"CULTURAL & VALUE COMPONENT - COMPLETE ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Analyze each cultural variable\n",
    "for var in cultural_vars:\n",
    "    print(f\"\\n{'=' * 100}\")\n",
    "    print(f\"{var.upper().replace('CULTURAL_', '')}\")\n",
    "    print(f\"{'=' * 100}\")\n",
    "    \n",
    "    # Clean data: remove None/NaN values\n",
    "    clean_data = df[var].dropna()\n",
    "    \n",
    "    print(f\"Total responses: {len(clean_data)} (Missing: {df[var].isnull().sum()})\")\n",
    "    print(f\"\\nValue counts:\")\n",
    "    value_counts = clean_data.value_counts()\n",
    "    value_pct = clean_data.value_counts(normalize=True) * 100\n",
    "    \n",
    "    for value, count in value_counts.items():\n",
    "        print(f\"  {value}: {count} ({value_pct[value]:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nUnique values: {clean_data.nunique()}\")\n",
    "    print(f\"Available values: {sorted(clean_data.unique().tolist())}\")\n",
    "\n",
    "# MORAL TRAITS ANALYSIS (List variable)\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"MORAL TRAITS (LIST VARIABLE)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Extract all moral traits from lists\n",
    "all_moral_traits = []\n",
    "for traits_list in df['cultural_moral_traits'].dropna():\n",
    "    if isinstance(traits_list, list):\n",
    "        all_moral_traits.extend(traits_list)\n",
    "\n",
    "if all_moral_traits:\n",
    "    moral_counts = Counter(all_moral_traits)\n",
    "    print(f\"Total moral trait entries: {len(all_moral_traits)}\")\n",
    "    print(f\"Unique moral traits: {len(moral_counts)}\")\n",
    "    print(f\"\\nMoral traits frequency:\")\n",
    "    for trait, count in moral_counts.most_common():\n",
    "        pct = (count / len(all_moral_traits)) * 100\n",
    "        print(f\"  {trait}: {count} ({pct:.2f}%)\")\n",
    "    print(f\"\\nAll unique moral traits: {sorted(moral_counts.keys())}\")\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot first 5 cultural variables\n",
    "for idx, var in enumerate(cultural_vars):\n",
    "    clean_data = df[var].dropna()\n",
    "    value_counts = clean_data.value_counts()\n",
    "    \n",
    "    # Bar chart\n",
    "    axes[idx].bar(range(len(value_counts)), value_counts.values, \n",
    "                  color=sns.color_palette('Spectral', len(value_counts)))\n",
    "    axes[idx].set_xticks(range(len(value_counts)))\n",
    "    axes[idx].set_xticklabels(value_counts.index, rotation=45, ha='right', fontsize=9)\n",
    "    axes[idx].set_ylabel('Frequency', fontsize=10)\n",
    "    axes[idx].set_title(var.replace('cultural_', '').replace('_', ' ').title(), \n",
    "                        fontsize=12, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(value_counts.values):\n",
    "        axes[idx].text(i, v, str(v), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Plot moral traits in the 6th subplot\n",
    "if all_moral_traits:\n",
    "    moral_counts_series = pd.Series(moral_counts).sort_values(ascending=False).head(15)\n",
    "    axes[5].barh(range(len(moral_counts_series)), moral_counts_series.values,\n",
    "                 color=sns.color_palette('coolwarm', len(moral_counts_series)))\n",
    "    axes[5].set_yticks(range(len(moral_counts_series)))\n",
    "    axes[5].set_yticklabels(moral_counts_series.index, fontsize=9)\n",
    "    axes[5].set_xlabel('Frequency', fontsize=10)\n",
    "    axes[5].set_title('Top 15 Moral Traits', fontsize=12, fontweight='bold')\n",
    "    axes[5].grid(True, alpha=0.3, axis='x')\n",
    "    axes[5].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cross-tabulation: Religion vs Religiosity Level\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"CROSS-TABULATION: Religion vs Religiosity Level\")\n",
    "print(\"=\" * 100)\n",
    "crosstab = pd.crosstab(df['cultural_religion'], df['cultural_religiosity_level'], margins=True)\n",
    "print(crosstab.to_string())\n",
    "\n",
    "# Ethnicity vs Language\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"CROSS-TABULATION: Ethnicity vs Language\")\n",
    "print(\"=\" * 100)\n",
    "crosstab2 = pd.crosstab(df['cultural_ethnicity'], df['cultural_language'], margins=True)\n",
    "print(crosstab2.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139d7252",
   "metadata": {},
   "source": [
    "## 10. Contextual Component Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a68109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contextual component variables (excluding list variables)\n",
    "context_vars = ['context_life_satisfaction', 'context_meaning_purpose']\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"CONTEXTUAL COMPONENT - COMPLETE ANALYSIS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Analyze each contextual variable\n",
    "for var in context_vars:\n",
    "    print(f\"\\n{'=' * 100}\")\n",
    "    print(f\"{var.upper().replace('CONTEXT_', '')}\")\n",
    "    print(f\"{'=' * 100}\")\n",
    "    \n",
    "    # Clean data: remove None/NaN values\n",
    "    clean_data = df[var].dropna()\n",
    "    \n",
    "    print(f\"Total responses: {len(clean_data)} (Missing: {df[var].isnull().sum()})\")\n",
    "    print(f\"\\nValue counts:\")\n",
    "    value_counts = clean_data.value_counts()\n",
    "    value_pct = clean_data.value_counts(normalize=True) * 100\n",
    "    \n",
    "    for value, count in value_counts.items():\n",
    "        print(f\"  {value}: {count} ({value_pct[value]:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\nUnique values: {clean_data.nunique()}\")\n",
    "    print(f\"Available values: {sorted(clean_data.unique().tolist())}\")\n",
    "\n",
    "# PERSONAL EXPERIENCES ANALYSIS (List variable)\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"PERSONAL EXPERIENCES (LIST VARIABLE)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Extract all personal experiences from lists\n",
    "all_personal_experiences = []\n",
    "for exp_list in df['context_personal_experiences'].dropna():\n",
    "    if isinstance(exp_list, list):\n",
    "        all_personal_experiences.extend(exp_list)\n",
    "\n",
    "if all_personal_experiences:\n",
    "    exp_counts = Counter(all_personal_experiences)\n",
    "    print(f\"Total personal experience entries: {len(all_personal_experiences)}\")\n",
    "    print(f\"Unique personal experiences: {len(exp_counts)}\")\n",
    "    print(f\"\\nPersonal experiences frequency:\")\n",
    "    for exp, count in exp_counts.most_common():\n",
    "        pct = (count / len(all_personal_experiences)) * 100\n",
    "        print(f\"  {exp}: {count} ({pct:.2f}%)\")\n",
    "    print(f\"\\nAll unique personal experiences: {sorted(exp_counts.keys())}\")\n",
    "\n",
    "# HISTORICAL EVENTS ANALYSIS (List variable)\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"HISTORICAL EVENTS (LIST VARIABLE)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Extract all historical events from lists\n",
    "all_historical_events = []\n",
    "for events_list in df['context_historical_events'].dropna():\n",
    "    if isinstance(events_list, list):\n",
    "        all_historical_events.extend(events_list)\n",
    "\n",
    "if all_historical_events:\n",
    "    events_counts = Counter(all_historical_events)\n",
    "    print(f\"Total historical event entries: {len(all_historical_events)}\")\n",
    "    print(f\"Unique historical events: {len(events_counts)}\")\n",
    "    print(f\"\\nHistorical events frequency:\")\n",
    "    for event, count in events_counts.most_common():\n",
    "        pct = (count / len(all_historical_events)) * 100\n",
    "        print(f\"  {event}: {count} ({pct:.2f}%)\")\n",
    "    print(f\"\\nAll unique historical events: {sorted(events_counts.keys())}\")\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "\n",
    "# Life Satisfaction\n",
    "clean_data = df['context_life_satisfaction'].dropna()\n",
    "value_counts = clean_data.value_counts()\n",
    "axes[0,0].bar(range(len(value_counts)), value_counts.values, \n",
    "              color=sns.color_palette('Set1', len(value_counts)))\n",
    "axes[0,0].set_xticks(range(len(value_counts)))\n",
    "axes[0,0].set_xticklabels(value_counts.index, rotation=45, ha='right', fontsize=9)\n",
    "axes[0,0].set_ylabel('Frequency', fontsize=10)\n",
    "axes[0,0].set_title('Life Satisfaction', fontsize=12, fontweight='bold')\n",
    "axes[0,0].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(value_counts.values):\n",
    "    axes[0,0].text(i, v, str(v), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Meaning and Purpose\n",
    "clean_data = df['context_meaning_purpose'].dropna()\n",
    "value_counts = clean_data.value_counts()\n",
    "axes[0,1].bar(range(len(value_counts)), value_counts.values, \n",
    "              color=sns.color_palette('Set2', len(value_counts)))\n",
    "axes[0,1].set_xticks(range(len(value_counts)))\n",
    "axes[0,1].set_xticklabels(value_counts.index, rotation=45, ha='right', fontsize=9)\n",
    "axes[0,1].set_ylabel('Frequency', fontsize=10)\n",
    "axes[0,1].set_title('Meaning and Purpose', fontsize=12, fontweight='bold')\n",
    "axes[0,1].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(value_counts.values):\n",
    "    axes[0,1].text(i, v, str(v), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Personal Experiences\n",
    "if all_personal_experiences:\n",
    "    exp_counts_series = pd.Series(exp_counts).sort_values(ascending=False).head(10)\n",
    "    axes[1,0].barh(range(len(exp_counts_series)), exp_counts_series.values,\n",
    "                   color=sns.color_palette('viridis', len(exp_counts_series)))\n",
    "    axes[1,0].set_yticks(range(len(exp_counts_series)))\n",
    "    axes[1,0].set_yticklabels(exp_counts_series.index, fontsize=9)\n",
    "    axes[1,0].set_xlabel('Frequency', fontsize=10)\n",
    "    axes[1,0].set_title('Top 10 Personal Experiences', fontsize=12, fontweight='bold')\n",
    "    axes[1,0].grid(True, alpha=0.3, axis='x')\n",
    "    axes[1,0].invert_yaxis()\n",
    "\n",
    "# Historical Events\n",
    "if all_historical_events:\n",
    "    events_counts_series = pd.Series(events_counts).sort_values(ascending=False)\n",
    "    axes[1,1].bar(range(len(events_counts_series)), events_counts_series.values,\n",
    "                  color=sns.color_palette('plasma', len(events_counts_series)))\n",
    "    axes[1,1].set_xticks(range(len(events_counts_series)))\n",
    "    axes[1,1].set_xticklabels(events_counts_series.index, rotation=45, ha='right', fontsize=9)\n",
    "    axes[1,1].set_ylabel('Frequency', fontsize=10)\n",
    "    axes[1,1].set_title('Historical Events', fontsize=12, fontweight='bold')\n",
    "    axes[1,1].grid(True, alpha=0.3, axis='y')\n",
    "    for i, v in enumerate(events_counts_series.values):\n",
    "        axes[1,1].text(i, v, str(v), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cross-tabulation: Life Satisfaction vs Meaning and Purpose\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"CROSS-TABULATION: Life Satisfaction vs Meaning and Purpose\")\n",
    "print(\"=\" * 100)\n",
    "crosstab = pd.crosstab(df['context_life_satisfaction'], df['context_meaning_purpose'], margins=True)\n",
    "print(crosstab.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e18330",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cc8e193",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
