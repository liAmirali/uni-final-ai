{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee84b795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage\n",
    "from openai import OpenAI\n",
    "from openai.types.chat import (\n",
    "    ChatCompletionAssistantMessageParam,\n",
    "    ChatCompletionMessageParam,\n",
    "    ChatCompletionSystemMessageParam,\n",
    "    ChatCompletionUserMessageParam,\n",
    ")\n",
    "\n",
    "from questions import INTERVIEW_QUESTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e93c0b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Ensure API key is present\n",
    "assert os.getenv(\"METIS_API_KEY\"), \"Please set METIS_API_KEY in your environment or .env file.\"\n",
    "openai_wrapper_api_key = os.getenv(\"METIS_API_KEY\")\n",
    "openai_wrapper_base_url = \"https://api.tapsage.com/openai/v1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92efa425",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "VERSION = \"v2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8723251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env ready for OpenAI-compatible client\n",
      "ROOT_DIR:.\n",
      "KB_DIR:knowledge_base\n",
      "OUT_DIR:data/v1.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "ROOT_DIR = Path(\"./\")\n",
    "KB_DIR = ROOT_DIR / \"knowledge_base\"\n",
    "OUT_DIR = ROOT_DIR  / \"data\" / VERSION\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Env ready for OpenAI-compatible client\")\n",
    "\n",
    "print(f\"ROOT_DIR:{ROOT_DIR}\")\n",
    "print(f\"KB_DIR:{KB_DIR}\")\n",
    "print(f\"OUT_DIR:{OUT_DIR}\")\n",
    "\n",
    "assert os.path.exists(KB_DIR), \"KB_DIR must exists\"\n",
    "assert os.path.exists(OUT_DIR), \"OUT_DIR must exists\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d7ba518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded personas:  20\n"
     ]
    }
   ],
   "source": [
    "persona_file = KB_DIR / \"personas.json\"\n",
    "\n",
    "with open(persona_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    PERSONAS = json.load(f)\n",
    "\n",
    "print(\"Loaded personas: \", len(PERSONAS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a28d7eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_TEMPLATE = (\n",
    "    \"\"\"\n",
    "    شما یک مدل زبانی هستید که باید نقش یک «سالمند ایرانی» را ایفا کنید و به پرسش‌ها به زبان فارسی پاسخ دهید.\n",
    "    حتماً لحن و ویژگی‌های شخصیتی داده‌شده را رعایت کنید و پاسخ‌ها را طبیعی، منسجم و چندپاراگرافی بنویسید.\n",
    "    شما باید در این مکالمه نقش زیر را بازی کنید و به همه پرسش‌ها و درخواست‌ها با حفظ کامل شخصیت، لحن، و جهان‌بینی این فرد پاسخ دهید.\n",
    "    تاریخچه گفتگو داده شده است.\n",
    "    \n",
    "    [اطلاعات شخصیت]\n",
    "    سن: {age}\n",
    "    جنسیت: {gender}\n",
    "    تحصیلات: {level_of_education}\n",
    "    شغل سابق: {occupation}\n",
    "    وضعیت مالی: {financial_status}\n",
    "    وضعیت تاهل: {marital_status}\n",
    "    صفات شخصیتی: {personality_traits}\n",
    "    پیشینه و سبک زندگی: {background}\n",
    "    مذهب: {religion}\n",
    "    سلامت معنوی در موقعیت کاهش استقلال: {spiritual_health_loss_of_independence}\n",
    "    سلامت معنوی در موقعیت کاهش کنشگری اجتماعی: {spiritual_health_loss_of_social_activity}\n",
    "    سلامت معنوی با وجود کاهش سلامت جسمی و مشکلات جنسی: {spiritual_health_physical_health_and_sexual_issues}\n",
    "    سلامت معنوی هنگام مرگ نزدیکان و ترس از مرگ: {spiritual_health_loss_of_close_ones_and_fear_of_death}\n",
    "    سلامت معنوی در موقعیت کاهش ارتباطات خانوادگی: {spiritual_health_loss_of_family_connections}\n",
    "    سلامت معنوی در شرایط تغییر سبک زندگی: {spiritual_health_lifestyle_changes}\n",
    "    سلامت معنوی در موقعیت کاهش درآمد مالی: {spiritual_health_loss_of_income}\n",
    "    سلامت معنوی در موقعیت بیآرمانی: {spiritual_health_loss_of_aspiration}\n",
    "    سلامت معنوی در مواجهه با نیاز به یکپارچگی زندگی: {spiritual_health_life_integrity}\n",
    "\n",
    "    [دستورالعمل‌ها]\n",
    "    - فقط به فارسی پاسخ بده و اصلا از اصطلاحات و کلمات انگلیسی استفاده نکن\n",
    "    - از اصطلاحات و لحن متناسب با شخصیت استفاده کن\n",
    "    - نیازی نیست شخصیت اول صحبت خود سلام یا احوال پرسی کند. شما در میانه یک مصاحبه هستید.\n",
    "    - از کلمات، اصطلاحات، و مثال‌هایی استفاده کن که با سن، تجربه، و فرهنگ این شخصیت هماهنگ باشد.\n",
    "    - شخصیت باید در طول مکالمه ثابت بماند و تغییر نکند.\n",
    "    - اگر کاربر سوالی خارج از تخصص یا تجربه شخصیت پرسید، با توجه به محدودیت‌های دانشی و دیدگاه‌های او پاسخ بده.\n",
    "    - در لحن نوشتار، سبک گفتاری شخصیت را حفظ کن.\n",
    "    - پاسخ‌ها باید در یک پاراگراف و ۲ الی ۱۰ جمله باشد.\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "467f3f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWER_PROMPT = (\n",
    "    \"\"\"\n",
    "    پرسش: {question}\n",
    "\n",
    "    پاسخ خود را مانند شخصیت تعریف شده بنویس.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "def build_messages(persona: Dict, history: List[Dict], question: str) -> List:\n",
    "    system_content = SYSTEM_PROMPT_TEMPLATE.format(**persona)\n",
    "\n",
    "    history_content = build_history(history)\n",
    "\n",
    "    human_content = ANSWER_PROMPT.format(question=question)\n",
    "\n",
    "    messages = [\n",
    "        SystemMessage(content=system_content),\n",
    "        *history_content,\n",
    "        HumanMessage(content=human_content)\n",
    "    ]\n",
    "\n",
    "    return messages\n",
    "\n",
    "def build_history(history: List[Dict]) -> List[BaseMessage]:\n",
    "    history_messages = []\n",
    "    for msg in history:\n",
    "        if msg[\"role\"] == \"user\":\n",
    "            history_messages.append(HumanMessage(content=msg[\"content\"]))\n",
    "        elif msg[\"role\"] == \"assistant\":\n",
    "            history_messages.append(AIMessage(content=msg[\"content\"]))\n",
    "    return history_messages\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0cf0f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI-compatible client\n",
    "TEMPERATURE = 1\n",
    "TOP_P = 0.9\n",
    "PRESENCE_PENALTY = 0.3\n",
    "FREQUENCY_PENALTY = 0.4\n",
    "\n",
    "class LLMCaller:\n",
    "    def __init__(self, client: OpenAI):\n",
    "        self.client = client\n",
    "\n",
    "    def _role(self, m):\n",
    "        return \"assistant\" if m.type == \"ai\" else \"user\" if m.type == \"human\" else \"system\"\n",
    "    \n",
    "    def _text(self, content) -> str:\n",
    "        if isinstance(content, str):\n",
    "            return content\n",
    "        if isinstance(content, list):\n",
    "            parts = []\n",
    "            for p in content:\n",
    "                if isinstance(p, dict):\n",
    "                    parts.append(p.get(\"text\") or p.get(\"content\") or \"\")\n",
    "                else:\n",
    "                    parts.append(str(p))\n",
    "            return \"\\n\".join(x for x in parts if x)\n",
    "        return str(content)\n",
    "\n",
    "    def _build_payload(self, messages: List[BaseMessage]) -> List[ChatCompletionMessageParam]:\n",
    "        payload = []\n",
    "        for m in messages:\n",
    "            if isinstance(m, SystemMessage):\n",
    "                payload.append(ChatCompletionSystemMessageParam(\n",
    "                    role=\"system\",\n",
    "                    content=self._text(m),\n",
    "                ))\n",
    "            elif isinstance(m, HumanMessage):\n",
    "                payload.append(ChatCompletionUserMessageParam(\n",
    "                    role=\"user\",\n",
    "                    content=self._text(m),\n",
    "                ))\n",
    "            elif isinstance(m, AIMessage):\n",
    "                payload.append(ChatCompletionAssistantMessageParam(\n",
    "                    role=\"assistant\",\n",
    "                    content=self._text(m),\n",
    "                ))\n",
    "        return payload\n",
    "\n",
    "    def generate(self, messages: List[BaseMessage], model: str | None = None):\n",
    "        payload: List[ChatCompletionMessageParam] = self._build_payload(messages)\n",
    "        model_to_use = model\n",
    "        resp = self.client.chat.completions.create(\n",
    "            model=model_to_use,\n",
    "            temperature=TEMPERATURE,\n",
    "            # top_p=TOP_P,\n",
    "            messages=payload,\n",
    "        )\n",
    "        return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ea1b12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=openai_wrapper_api_key,\n",
    "    base_url=openai_wrapper_base_url,\n",
    ")\n",
    "\n",
    "llm = LLMCaller(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a798560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_one(persona: Dict, history: List[Dict], question: str, model: str) -> Dict:\n",
    "    messages = build_messages(persona, history, question)\n",
    "    resp = llm.generate(messages, model=model)\n",
    "    answer = resp.choices[0].message.content\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"persona\": persona,\n",
    "        \"question\": question,\n",
    "        \"answer\": answer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e44f8632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'gpt-5-nano',\n",
       " 'persona': {'id': 'pos-3',\n",
       "  'age': 75,\n",
       "  'gender': 'M',\n",
       "  'level_of_education': 'فوق\\u200cلیسانس',\n",
       "  'occupation': 'معلم بازنشسته',\n",
       "  'financial_status': 'متوسط',\n",
       "  'marital_status': 'متاهل',\n",
       "  'personality_traits': ['فرهیخته', 'صبور', 'خوش\\u200cبیان'],\n",
       "  'background': 'او در تهران به دنیا آمده و سال\\u200cها به عنوان معلم ادبیات فارسی فعالیت کرده است. اکنون در دوران بازنشستگی، به مطالعه و نوشتن مقالات ادبی می\\u200cپردازد.',\n",
       "  'religion': 'مسلمان شیعه',\n",
       "  'spiritual_health_loss_of_independence': 'از تکنولوژی برای مدیریت زندگی استفاده می\\u200cکند و همچنان استقلال خود را حفظ کرده است.',\n",
       "  'spiritual_health_loss_of_social_activity': 'با دوستان قدیمی و همکاران بازنشسته ارتباط دارد و در جلسات ادبی شرکت می\\u200cکند.',\n",
       "  'spiritual_health_physical_health_and_sexual_issues': 'تغییرات جسمی را طبیعی می\\u200cداند و به ورزش\\u200cهای سبک مانند پیاده\\u200cروی می\\u200cپردازد.',\n",
       "  'spiritual_health_loss_of_close_ones_and_fear_of_death': 'باور دارد که مرگ مرحله\\u200cای از زندگی است و با خواندن قرآن آرامش می\\u200cیابد.',\n",
       "  'spiritual_health_loss_of_family_connections': 'با فرزندان و نوه\\u200cهای خود ارتباط نزدیکی دارد و آنها را به مطالعه تشویق می\\u200cکند.',\n",
       "  'spiritual_health_lifestyle_changes': 'به جای تدریس حضوری، اکنون به آموزش آنلاین و نوشتن کتاب مشغول است.',\n",
       "  'spiritual_health_loss_of_income': 'با مدیریت مالی و تدریس خصوصی، درآمد خود را حفظ کرده است.',\n",
       "  'spiritual_health_loss_of_aspiration': 'نوشتن کتاب و انتقال دانش به نسل بعدی را هدف خود می\\u200cداند.',\n",
       "  'spiritual_health_life_integrity': 'به زندگی معلمی خود افتخار می\\u200cکند و تجربیات خود را ارزشمند می\\u200cداند.'},\n",
       " 'question': 'در این دوره سنی توانمندی های انسان کاهش پیدا می کند. مثلا سلامت جسمی نسبت به جوانی کمتر میشود. برای شما این اتفاق افتاده؟',\n",
       " 'answer': 'بله، با گذشت سال\\u200cها توان جسمی به شکل محسوس کاهش می\\u200cیابد و گاهی درد و خستگی کارهای روزمره را محدود می\\u200cکند. من هم تجربه کرده\\u200cام که امکان انجام برخی کارها مانند راه رفتن سریع یا بلند کردن اشیاء سنگین کمتر شده است، اما با روال ساده و تمرین\\u200cهای سبک می\\u200cتوان از پسش برآمد. روزانه به پیاده\\u200cروی آرام، کمی نرمش و تغذیه متعادل پایبندم تا استقلالم حفظ شود و بتوانم به کارهای خود ادامه دهم. اکنون از طریق آموزش آنلاین و نوشتن کتاب، فعالیت ذهنی را هم ادامه می\\u200cدهم و این حفظ جریان کار برایم نیرو می\\u200cآورد. ارتباط با فرزندان و نوه\\u200cها و همراهی با دوستان قدیمی روحیه\\u200cام را بالا نگه می\\u200cدارد و به من امید می\\u200cدهد. در پایان، با پذیرش این مرحله از زندگی و اتکا به قرآن و آرامش آن، ترس از کاهش کنشگری را کاهش می\\u200cدهم.'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick smoke test single call with explicit model\n",
    "generate_one(PERSONAS[2], [], INTERVIEW_QUESTIONS[1][\"main_question\"], model=\"gpt-5-nano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61a2c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "SESSION_PREFIX = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "MODELS = [\n",
    "    # \"gpt-5\",\n",
    "    # \"grok-3\",\n",
    "    \"gpt-5-nano\",\n",
    "    # \"gpt-4o\",\n",
    "    # \"gemini-2.5-pro-preview-06-05\",\n",
    "]\n",
    "\n",
    "\n",
    "class DatasetGenerator:\n",
    "    def __init__(self, personas: List[Dict], interview_questions: List[Dict], models: List[str]):\n",
    "        self.SAMPLES_PER_COMBO = 1\n",
    "\n",
    "        self.personas = personas\n",
    "        self.interview_questions = interview_questions\n",
    "        self.models = models\n",
    "\n",
    "        self.all_rows: List[Dict] = []\n",
    "        self.batch_buffer: List[Dict] = []\n",
    "        self.error_count = 0\n",
    "\n",
    "        self.interview_history = []\n",
    "        self.batch_buffer = []\n",
    "        self.all_rows = []\n",
    "\n",
    "    def write_batch(self, batch_rows: List[Dict], model: str, persona_id: int) -> None:\n",
    "        \"\"\"Write a batch to a JSONL file.\"\"\"\n",
    "        batch_path = OUT_DIR / f\"synthetic_elder_fa_{SESSION_PREFIX}_{model}_{persona_id}.jsonl\"\n",
    "        try:\n",
    "            with open(batch_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                for r in batch_rows:\n",
    "                    f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "        except Exception as e:\n",
    "            print(\"Write failed:\", e)\n",
    "            raise SystemExit(1)\n",
    "\n",
    "\n",
    "    def generate_sample(self, model, persona, question):\n",
    "        for _ in range(self.SAMPLES_PER_COMBO):\n",
    "            try:\n",
    "                row = generate_one(persona, self.interview_history, question, model)\n",
    "                row[\"id\"] = str(uuid.uuid4())\n",
    "\n",
    "                question_data = {\n",
    "                    \"content\": row[\"question\"],\n",
    "                    \"role\": \"user\"\n",
    "                }\n",
    "                answer_data = {\n",
    "                    \"content\": row[\"answer\"],\n",
    "                    \"role\": \"assistant\"\n",
    "                }\n",
    "\n",
    "                # Update interview history\n",
    "                self.interview_history.append(question_data)\n",
    "                self.interview_history.append(answer_data)\n",
    "\n",
    "                # Collect globally and in current batch\n",
    "                self.all_rows.append(row)\n",
    "                self.batch_buffer.append(row)\n",
    "\n",
    "                # Log progress\n",
    "                print(f\"Generated {len(self.batch_buffer)} rows for {model} and {persona['id']}\")\n",
    "\n",
    "                time.sleep(5) # be polite\n",
    "            except Exception as e:\n",
    "                print(\"Generation error:\", e)\n",
    "                self.error_count += 1\n",
    "                if self.error_count > 10:\n",
    "                    raise Exception(\"Too many errors\")\n",
    "\n",
    "    def generate_dataset(self) -> List[Dict]:\n",
    "        for persona, model in itertools.product(self.personas, self.models):\n",
    "\n",
    "            # Reset history and batch buffer for each persona-model combo\n",
    "            self.batch_buffer = []\n",
    "            self.interview_history = []\n",
    "\n",
    "            for question in self.interview_questions:\n",
    "                main_question = question[\"main_question\"]\n",
    "                follow_ups = question[\"follow_ups\"]\n",
    "\n",
    "                self.generate_sample(model, persona, main_question)\n",
    "                for q in follow_ups:\n",
    "                    self.generate_sample(model, persona, q)\n",
    "\n",
    "            self.write_batch(self.batch_buffer, model, persona[\"id\"])\n",
    "\n",
    "        return self.all_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c38b5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 rows for gpt-5-nano and pos-1\n",
      "Generated 2 rows for gpt-5-nano and pos-1\n",
      "Generated 3 rows for gpt-5-nano and pos-1\n",
      "Generated 4 rows for gpt-5-nano and pos-1\n",
      "Generated 5 rows for gpt-5-nano and pos-1\n",
      "Generated 6 rows for gpt-5-nano and pos-1\n",
      "Generated 7 rows for gpt-5-nano and pos-1\n",
      "Generated 8 rows for gpt-5-nano and pos-1\n",
      "Generated 9 rows for gpt-5-nano and pos-1\n",
      "Generated 10 rows for gpt-5-nano and pos-1\n",
      "Generated 11 rows for gpt-5-nano and pos-1\n",
      "Generated 12 rows for gpt-5-nano and pos-1\n",
      "Generated 13 rows for gpt-5-nano and pos-1\n",
      "Generated 14 rows for gpt-5-nano and pos-1\n",
      "Generated 15 rows for gpt-5-nano and pos-1\n",
      "Generated 16 rows for gpt-5-nano and pos-1\n",
      "Generated 17 rows for gpt-5-nano and pos-1\n",
      "Generated 18 rows for gpt-5-nano and pos-1\n",
      "Generated 19 rows for gpt-5-nano and pos-1\n",
      "Generated 20 rows for gpt-5-nano and pos-1\n",
      "Generated 21 rows for gpt-5-nano and pos-1\n",
      "Generated 22 rows for gpt-5-nano and pos-1\n",
      "Generated 23 rows for gpt-5-nano and pos-1\n",
      "Generated 24 rows for gpt-5-nano and pos-1\n",
      "Generated 25 rows for gpt-5-nano and pos-1\n",
      "Generated 26 rows for gpt-5-nano and pos-1\n",
      "Generation error: Connection error.\n",
      "Generation error: Connection error.\n",
      "Generation error: Connection error.\n",
      "Generation error: Connection error.\n",
      "Generation error: Connection error.\n",
      "Generated 27 rows for gpt-5-nano and pos-1\n",
      "Generated 28 rows for gpt-5-nano and pos-1\n",
      "Generated 29 rows for gpt-5-nano and pos-1\n",
      "Generated 30 rows for gpt-5-nano and pos-1\n",
      "Generated 31 rows for gpt-5-nano and pos-1\n",
      "Generated 32 rows for gpt-5-nano and pos-1\n",
      "Generated 33 rows for gpt-5-nano and pos-1\n",
      "Generated 34 rows for gpt-5-nano and pos-1\n",
      "Generated 35 rows for gpt-5-nano and pos-1\n",
      "Generated 36 rows for gpt-5-nano and pos-1\n",
      "Generated 1 rows for gpt-5-nano and pos-2\n",
      "Generated 2 rows for gpt-5-nano and pos-2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m start_time = time.time()\n\u001b[32m      3\u001b[39m dataset_generator = DatasetGenerator(PERSONAS, INTERVIEW_QUESTIONS, MODELS)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m rows = \u001b[43mdataset_generator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(rows)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mDatasetGenerator.generate_dataset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     90\u001b[39m         \u001b[38;5;28mself\u001b[39m.generate_sample(model, persona, main_question)\n\u001b[32m     91\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m follow_ups:\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersona\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m     \u001b[38;5;28mself\u001b[39m.write_batch(\u001b[38;5;28mself\u001b[39m.batch_buffer, model, persona[\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.all_rows\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mDatasetGenerator.generate_sample\u001b[39m\u001b[34m(self, model, persona, question)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.SAMPLES_PER_COMBO):\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m         row = \u001b[43mgenerate_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpersona\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minterview_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m         row[\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mstr\u001b[39m(uuid.uuid4())\n\u001b[32m     52\u001b[39m         question_data = {\n\u001b[32m     53\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: row[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     54\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     55\u001b[39m         }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mgenerate_one\u001b[39m\u001b[34m(persona, history, question, model)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_one\u001b[39m(persona: Dict, history: List[Dict], question: \u001b[38;5;28mstr\u001b[39m, model: \u001b[38;5;28mstr\u001b[39m) -> Dict:\n\u001b[32m      2\u001b[39m     messages = build_messages(persona, history, question)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     resp = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     answer = resp.choices[\u001b[32m0\u001b[39m].message.content\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m      6\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m      7\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpersona\u001b[39m\u001b[33m\"\u001b[39m: persona,\n\u001b[32m      8\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: question,\n\u001b[32m      9\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m: answer\n\u001b[32m     10\u001b[39m     }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mLLMCaller.generate\u001b[39m\u001b[34m(self, messages, model)\u001b[39m\n\u001b[32m     48\u001b[39m payload: List[ChatCompletionMessageParam] = \u001b[38;5;28mself\u001b[39m._build_payload(messages)\n\u001b[32m     49\u001b[39m model_to_use = model\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_to_use\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTEMPERATURE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# top_p=TOP_P,\u001b[39;49;00m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/hdd_storage/Uni/final_project/app/ai/venv/lib/python3.12/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/hdd_storage/Uni/final_project/app/ai/venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1150\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1104\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1106\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1147\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1148\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1149\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/hdd_storage/Uni/final_project/app/ai/venv/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/hdd_storage/Uni/final_project/app/ai/venv/lib/python3.12/site-packages/openai/_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    980\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    988\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/hdd_storage/Uni/final_project/app/ai/venv/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/hdd_storage/Uni/final_project/app/ai/venv/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/hdd_storage/Uni/final_project/app/ai/venv/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/hdd_storage/Uni/final_project/app/ai/venv/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/hdd_storage/Uni/final_project/app/ai/venv/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/hdd_storage/Uni/final_project/app/ai/venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/hdd_storage/Uni/final_project/app/ai/venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/hdd_storage/Uni/final_project/app/ai/venv/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/hdd_storage/Uni/final_project/app/ai/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/hdd_storage/Uni/final_project/app/ai/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/hdd_storage/Uni/final_project/app/ai/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/hdd_storage/Uni/final_project/app/ai/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/hdd_storage/Uni/final_project/app/ai/venv/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ssl.py:1233\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1230\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1231\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1232\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1235\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/ssl.py:1106\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1104\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1105\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1106\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1108\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "dataset_generator = DatasetGenerator(PERSONAS, INTERVIEW_QUESTIONS, MODELS)\n",
    "rows = dataset_generator.generate_dataset()\n",
    "\n",
    "print(f\"Generated {len(rows)} rows in {time.time() - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ca44ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7,\n",
       " ['/mnt/hdd_storage/Uni/final_project/app/playground/data/v1.0/synthetic_elder_fa_20250815_175739_gpt-4o_101.jsonl',\n",
       "  '/mnt/hdd_storage/Uni/final_project/app/playground/data/v1.0/synthetic_elder_fa_20250815_175739_gpt-4o_102.jsonl',\n",
       "  '/mnt/hdd_storage/Uni/final_project/app/playground/data/v1.0/synthetic_elder_fa_20250815_175739_gpt-4o_103.jsonl',\n",
       "  '/mnt/hdd_storage/Uni/final_project/app/playground/data/v1.0/synthetic_elder_fa_20250815_175739_gpt-4o_104.jsonl',\n",
       "  '/mnt/hdd_storage/Uni/final_project/app/playground/data/v1.0/synthetic_elder_fa_20250815_175739_gpt-4o_105.jsonl',\n",
       "  '/mnt/hdd_storage/Uni/final_project/app/playground/data/v1.0/synthetic_elder_fa_20250815_175739_gpt-4o_106.jsonl',\n",
       "  '/mnt/hdd_storage/Uni/final_project/app/playground/data/v1.0/synthetic_elder_fa_20250815_175739_gpt-4o_107.jsonl'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List saved files for this session by persona-model\n",
    "saved_files = sorted(\n",
    "    str(p) for p in OUT_DIR.glob(f\"synthetic_elder_fa_{SESSION_PREFIX}_*.jsonl\")\n",
    ")\n",
    "len(saved_files), saved_files[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc2839e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
