{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5215de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from persona_generation_prompt import PROMPT\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "985e8edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonaDetails():\n",
    "    def __init__(self, age, gender, marital_status, children, living_situation, general_health, chronic_disease, mobility, hearing_senses, vision_senses, daily_energy, personality_type, cognitive_status, dominant_emotion, emotional_intelligence, iq, attitude_toward_aging, main_social_role, social_support, social_participation, income, economic_decile, housing, religion_and_sect, internalized_moral_traits, religiosity_level, ethnicity, language, important_personal_experiences, life_satisfaction, meaning_and_purpose_in_old_age):\n",
    "        self.age = age\n",
    "        self.gender = gender\n",
    "        self.marital_status = marital_status\n",
    "        self.children = children\n",
    "        self.living_situation = living_situation\n",
    "        self.general_health = general_health\n",
    "        self.chronic_disease = chronic_disease\n",
    "        self.mobility = mobility\n",
    "        self.hearing_senses = hearing_senses\n",
    "        self.vision_senses = vision_senses\n",
    "        self.daily_energy = daily_energy\n",
    "        self.personality_type = personality_type\n",
    "        self.cognitive_status = cognitive_status\n",
    "        self.dominant_emotion = dominant_emotion\n",
    "        self.emotional_intelligence = emotional_intelligence\n",
    "        self.iq = iq\n",
    "        self.attitude_toward_aging = attitude_toward_aging\n",
    "        self.main_social_role = main_social_role\n",
    "        self.social_support = social_support\n",
    "        self.social_participation = social_participation\n",
    "        self.income = income\n",
    "        self.economic_decile = economic_decile\n",
    "        self.housing = housing\n",
    "        self.religion_and_sect = religion_and_sect\n",
    "        self.internalized_moral_traits = internalized_moral_traits\n",
    "        self.religiosity_level = religiosity_level\n",
    "        self.ethnicity = ethnicity\n",
    "        self.language = language\n",
    "        self.important_personal_experiences = important_personal_experiences\n",
    "        self.life_satisfaction = life_satisfaction\n",
    "        self.meaning_and_purpose_in_old_age = meaning_and_purpose_in_old_age\n",
    "\n",
    "    def to_json(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, sort_keys=True, indent=4)\n",
    "\n",
    "    def to_dict(self):\n",
    "        return self.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200cb017",
   "metadata": {},
   "source": [
    "# Age Pyramid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb388d61",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook provides two approaches for generating Iranian elderly personas:\n",
    "\n",
    "### 1. **Statistical Base Persona Generation** (New - Recommended)\n",
    "Generate personas with demographics based on real Iranian population statistics:\n",
    "- **Age distribution**: Follows decaying distribution for elderly (65-95)\n",
    "- **Gender distribution**: 53% Female, 47% Male\n",
    "- **Ethnicity/Language**: Reflects Iranian ethnic diversity (Persian, Azeri, Kurdish, etc.)\n",
    "- **Religion**: Matches ethnicity-based religious distributions\n",
    "- **Marital status, Children, Living situation**: Based on Iranian family patterns\n",
    "\n",
    "The LLM then fills in psychological, social, economic, and contextual fields that are consistent with these demographics.\n",
    "\n",
    "### 2. **Full LLM Generation** (Original)\n",
    "Let the LLM generate all fields from scratch based on the full prompt.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41c430f",
   "metadata": {},
   "source": [
    "## Quick Reference\n",
    "\n",
    "### For Statistical Base Persona Generation:\n",
    "\n",
    "```python\n",
    "# Synchronous (for testing):\n",
    "personas = generate_personas_with_stats_sync(personas_count=10)\n",
    "\n",
    "# Asynchronous batch (for large scale):\n",
    "batch = call_llm_batch_with_base_personas(\n",
    "    model=MODEL_TO_USE,\n",
    "    personas_per_batch=10,\n",
    "    batch_count=20  # Total: 200 personas\n",
    ")\n",
    "```\n",
    "\n",
    "### For Full LLM Generation:\n",
    "\n",
    "```python\n",
    "# Original approach:\n",
    "batch = call_llm_batch(MODEL_TO_USE, messages(10), 20)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37eea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_base_persona_complete():\n",
    "    \"\"\"\n",
    "    Generate a base persona with statistically-based demographic fields.\n",
    "    Returns a dictionary with predefined fields based on Iranian elderly population statistics.\n",
    "    The LLM will fill in the remaining fields.\n",
    "    \"\"\"\n",
    "    # Gender distribution (F: 53%, M: 47%)\n",
    "    gender = random.choices([\"Female\", \"Male\"], weights=[53, 47])[0]\n",
    "    \n",
    "    # Age distribution (decaying distribution for elderly)\n",
    "    age = random.choices(range(65, 95), weights=[25, 20, 15, 15, 10, 10, 5, 3, 2, 1])[0]\n",
    "    \n",
    "    # Marital status distribution\n",
    "    marital_status = random.choices([\"Married\", \"Single\", \"Divorced\", \"Widowed\"], weights=[60, 30, 5, 5])[0]\n",
    "    \n",
    "    # Children distribution\n",
    "    children = random.choices([\"None\", \"1\", \"2-3\", \"4+\"], weights=[5, 15, 30, 50])[0]\n",
    "    \n",
    "    # Living situation distribution\n",
    "    living_situation = random.choices([\"Living with Family\", \"Living Alone\", \"Shared Housing\"], weights=[50, 30, 20])[0]\n",
    "    \n",
    "    # Ethnicity distribution (approximate Iranian demographics)\n",
    "    ethnicity = random.choices(\n",
    "        [\"Persian\", \"Azeri\", \"Kurdish\", \"Lur\", \"Baloch\", \"Arab\", \"Turkmen\", \"Gilaki\", \"Mazandarani\", \"Qashqai\"],\n",
    "        weights=[50, 25, 10, 5, 3, 2, 1, 2, 1, 1]\n",
    "    )[0]\n",
    "    \n",
    "    # Language typically matches ethnicity\n",
    "    language_map = {\n",
    "        \"Persian\": \"Persian\", \"Azeri\": \"Azeri\", \"Kurdish\": \"Kurdish\",\n",
    "        \"Lur\": \"Luri\", \"Baloch\": \"Balochi\", \"Arab\": \"Arabic\",\n",
    "        \"Turkmen\": \"Turkmen\", \"Gilaki\": \"Gilaki\",\n",
    "        \"Mazandarani\": \"Mazandarani\", \"Qashqai\": \"Qashqai\"\n",
    "    }\n",
    "    language = language_map.get(ethnicity, \"Persian\")\n",
    "    \n",
    "    # Religion distribution\n",
    "    if ethnicity in [\"Persian\", \"Azeri\", \"Gilaki\", \"Mazandarani\"]:\n",
    "        religion = random.choices([\"Shia Muslim\", \"Sunni Muslim\"], weights=[95, 5])[0]\n",
    "    elif ethnicity in [\"Kurdish\", \"Baloch\", \"Turkmen\"]:\n",
    "        religion = random.choices([\"Sunni Muslim\", \"Shia Muslim\"], weights=[80, 20])[0]\n",
    "    elif ethnicity == \"Arab\":\n",
    "        religion = random.choices([\"Shia Muslim\", \"Sunni Muslim\"], weights=[70, 30])[0]\n",
    "    else:\n",
    "        religion = random.choices([\"Shia Muslim\", \"Sunni Muslim\", \"Zoroastrian\", \"Christian\", \"Jewish\"], \n",
    "                                 weights=[85, 10, 2, 2, 1])[0]\n",
    "    \n",
    "    return {\n",
    "        \"age\": age,\n",
    "        \"gender\": gender,\n",
    "        \"marital_status\": marital_status,\n",
    "        \"children\": children,\n",
    "        \"living_situation\": living_situation,\n",
    "        \"ethnicity\": ethnicity,\n",
    "        \"language\": language,\n",
    "        \"religion_and_sect\": religion\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a7d582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function\n",
    "base_persona_sample = generate_base_persona_complete()\n",
    "print(\"Sample base persona:\")\n",
    "print(json.dumps(base_persona_sample, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4523b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_constrained_prompt(base_personas: list):\n",
    "    \"\"\"\n",
    "    Create a prompt that asks the LLM to complete personas with predefined demographic fields.\n",
    "    \n",
    "    Args:\n",
    "        base_personas: List of dictionaries containing predefined demographic fields\n",
    "    \n",
    "    Returns:\n",
    "        str: Prompt for the LLM\n",
    "    \"\"\"\n",
    "    \n",
    "    base_prompt = \"\"\"You must complete the following Iranian elderly persona(s) by filling in the missing fields.\n",
    "\n",
    "Rules:\n",
    "- The demographic fields (age, gender, marital_status, children, living_situation, ethnicity, language, religion_and_sect) are already provided. DO NOT change them.\n",
    "- Fill in all the remaining fields with realistic values that are consistent with the provided demographic information.\n",
    "- Ensure diversity in the values you assign, but maintain realism and internal consistency.\n",
    "- Personas should reflect cultural and social realities of Iran.\n",
    "- Reactions and attitudes do not need to be \"correct\" or \"moral\"; they may be shaped by culture, personal experience, or limitations.\n",
    "- Personality traits and psychological states should be consistent with the individual's background.\n",
    "\n",
    "Fields to fill in:\n",
    "### Biological Component\n",
    "* **General Health**: [\"Good\", \"Average\", \"Poor\"]\n",
    "* **Chronic Disease**: [None, \"High Blood Pressure\", \"Cardiovascular Diseases\", \"Type 2 Diabetes\", \"Arthritis and Joint Pain\", \"Osteoporosis\", \"Alzheimer's and Dementia\", \"Chronic Kidney Disease\", \"Chronic Obstructive Pulmonary Disease\", \"Chronic Depression and Anxiety\", \"Vision and Hearing Problems\", \"Chronic Liver Failure\", \"Parkinson's\", \"Chronic Sleep Disorders\", \"Chronic Gastrointestinal Issues\"]\n",
    "* **Mobility**: [\"Independent\", \"With Cane or Walker\", \"In Wheelchair\", \"Dependent\"]\n",
    "* **Hearing Senses**: [\"Good\", \"Average\", \"Poor\"]\n",
    "* **Vision Senses**: [\"Good\", \"Average\", \"Poor\"]\n",
    "* **Daily Energy**: [\"High\", \"Average\", \"Low\"]\n",
    "\n",
    "### Psychological Component\n",
    "* **Personality Type**: [\"INTJ\", \"INTP\", \"ENTJ\", \"ENTP\", \"INFJ\", \"INFP\", \"ENFJ\", \"ENFP\", \"ISTJ\", \"ISFJ\", \"ESTJ\", \"ESFJ\", \"ISTP\", \"ISFP\", \"ESTP\", \"ESFP\"]\n",
    "* **Cognitive Status**: [\"Healthy Memory\", \"Mild Forgetfulness\", \"Alzheimer's\"]\n",
    "* **Dominant Emotion**: [\"Happy\", \"Sad\", \"Anxious\", \"Calm\"]\n",
    "* **Emotional Intelligence**: [\"Low\", \"Average\", \"High\"]\n",
    "* **IQ**: [\"Low\", \"Average\", \"High\"]\n",
    "* **Attitude Toward Aging**: [\"Acceptance\", \"Resistance\", \"Meaning-Seeking\", \"Denial\"]\n",
    "\n",
    "### Social Component\n",
    "* **Main Social Role**: [\"Grandfather\", \"Grandmother\", \"Retired\", \"Social Activist\"]\n",
    "* **Social Support**: [\"Large Family\", \"Alone\", \"Supportive Friends\", \"Government Support\"]\n",
    "* **Social Participation**: [\"Active\", \"Inactive\"]\n",
    "\n",
    "### Economic Component\n",
    "* **Income**: [\"Independent\", \"Retirement Pension\", \"Dependent on Children\", \"No Income\"]\n",
    "* **Economic Decile**: integer 1–10\n",
    "* **Housing**: [\"Own Home\", \"Rented\", \"Nursing Home\"]\n",
    "\n",
    "### Cultural-Value Component\n",
    "* **Internalized Moral Traits**: list of 2–4 traits (positive or negative)\n",
    "* **Religiosity Level**: [\"Low\", \"Average\", \"High\"]\n",
    "\n",
    "### Contextual Component\n",
    "* **Important Personal Experiences**: [\"Immigration\", \"Career Success\", \"Loss of Loved Ones\", \"War Experience\", \"Economic Hardship\", \"Educational Achievement\", \"Battle with Serious Illness (e.g., Cancer, Chronic Disease)\"]\n",
    "* **Life Satisfaction**: [\"Satisfied\", \"Dissatisfied\", \"Neutral\"]\n",
    "* **Meaning and Purpose in Old Age**: [\"Helping Family\", \"Spiritual Activities\", \"Waiting for Death\", \"Pleasure-Seeking\"]\n",
    "\n",
    "Base personas to complete:\n",
    "\"\"\"\n",
    "    \n",
    "    personas_str = json.dumps(base_personas, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return base_prompt + personas_str\n",
    "\n",
    "\n",
    "def messages_with_base(base_personas: list):\n",
    "    \"\"\"\n",
    "    Create messages for the LLM with base personas that need to be completed.\n",
    "    \n",
    "    Args:\n",
    "        base_personas: List of dictionaries containing predefined demographic fields\n",
    "    \n",
    "    Returns:\n",
    "        list: Messages for the LLM API\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": create_constrained_prompt(base_personas)\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Complete all {len(base_personas)} persona(s) by filling in the missing fields. Return ONLY the JSON array with complete personas, no extra text or markdown formatting.\"\n",
    "        }\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f14299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm_batch_with_base_personas(model, personas_per_batch=10, batch_count=1, batch_file_path=\"batch_input.jsonl\"):\n",
    "    \"\"\"\n",
    "    Call the OpenAI Batch API with pre-generated base personas.\n",
    "    \n",
    "    Args:\n",
    "        model (str): The model to use.\n",
    "        personas_per_batch (int): Number of personas per batch request.\n",
    "        batch_count (int): The number of batch requests to create.\n",
    "        batch_file_path (str): Path to save the batch input file.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Batch metadata including status and file IDs.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import os\n",
    "    \n",
    "    # Step 1: Prepare the batch input file\n",
    "    with open(batch_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i in range(batch_count):\n",
    "            # Generate base personas for this batch\n",
    "            base_personas = [generate_base_persona_complete() for _ in range(personas_per_batch)]\n",
    "            \n",
    "            # Create messages with these base personas\n",
    "            batch_messages = messages_with_base(base_personas)\n",
    "            \n",
    "            batch_request = {\n",
    "                \"custom_id\": f\"request-{i+1}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": model,\n",
    "                    \"messages\": batch_messages,\n",
    "                    \"temperature\": TEMPERATURE,\n",
    "                    \"top_p\": TOP_P,\n",
    "                    \"presence_penalty\": PRESENCE_PENALTY,\n",
    "                    \"frequency_penalty\": FREQUENCY_PENALTY\n",
    "                }\n",
    "            }\n",
    "            f.write(json.dumps(batch_request, ensure_ascii=False) + \"\\n\")\n",
    "    \n",
    "    # Step 2: Upload the batch input file\n",
    "    batch_input_file = client.files.create(\n",
    "        file=open(batch_file_path, \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "    \n",
    "    # Step 3: Create the batch\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=batch_input_file.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\n",
    "            \"description\": f\"Batch processing for {batch_count * personas_per_batch} personas with base demographics\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7014ab",
   "metadata": {},
   "source": [
    "# Example: Generate Personas with Predefined Statistics\n",
    "\n",
    "The new workflow:\n",
    "1. **`generate_base_persona_complete()`** - Generates demographic fields based on Iranian population statistics\n",
    "2. **`messages_with_base()`** - Creates messages that instruct the LLM to complete the persona\n",
    "3. **`call_llm_batch_with_base_personas()`** - Creates batch jobs with statistically-generated base personas\n",
    "\n",
    "This approach ensures:\n",
    "- Realistic demographic distributions\n",
    "- Consistency with Iranian elderly population statistics\n",
    "- LLM focuses on filling psychological, social, and contextual fields that are consistent with the demographics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75734cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Test with a small batch\n",
    "# Generate 3 base personas\n",
    "test_base_personas = [generate_base_persona_complete() for _ in range(3)]\n",
    "\n",
    "print(\"Generated base personas:\")\n",
    "for i, persona in enumerate(test_base_personas, 1):\n",
    "    print(f\"\\nPersona {i}:\")\n",
    "    print(json.dumps(persona, indent=2, ensure_ascii=False))\n",
    "\n",
    "# Show what the messages would look like\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample prompt sent to LLM:\")\n",
    "print(\"=\"*80)\n",
    "test_messages = messages_with_base([test_base_personas[0]])\n",
    "print(test_messages[0][\"content\"][:1000] + \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d481750",
   "metadata": {},
   "source": [
    "# Running Batch Jobs with Base Personas\n",
    "\n",
    "To generate personas with predefined statistics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf4f2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Generate 200 personas (20 batches of 10 personas each)\n",
    "# Uncomment to run:\n",
    "# batch_job = call_llm_batch_with_base_personas(\n",
    "#     model=MODEL_TO_USE,\n",
    "#     personas_per_batch=10,  # 10 personas per batch request\n",
    "#     batch_count=20,          # 20 batch requests = 200 total personas\n",
    "#     batch_file_path=\"batch_input_with_stats.jsonl\"\n",
    "# )\n",
    "# print(f\"Batch job created: {batch_job.id}\")\n",
    "# print(f\"Status: {batch_job.status}\")\n",
    "\n",
    "# To check status later:\n",
    "# result = poll_batch_status(batch_job)\n",
    "# if result:\n",
    "#     output_path = save_batch_output(batch_job)\n",
    "#     print(f\"Personas saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156a1b95",
   "metadata": {},
   "source": [
    "# Comparison: Old vs New Approach\n",
    "\n",
    "## Old Approach (Original `messages()` function)\n",
    "- LLM generates ALL fields including demographics\n",
    "- Less control over statistical distributions\n",
    "- May not accurately reflect Iranian elderly population statistics\n",
    "- Usage: `call_llm_batch(MODEL_TO_USE, messages(10), 20)`\n",
    "\n",
    "## New Approach (with `call_llm_batch_with_base_personas()`)\n",
    "- Demographics generated with realistic statistical distributions\n",
    "- Age, gender, marital status, ethnicity, language, religion follow Iranian population data\n",
    "- LLM focuses on psychological, social, and economic fields\n",
    "- More realistic and diverse personas\n",
    "- Usage: `call_llm_batch_with_base_personas(MODEL_TO_USE, personas_per_batch=10, batch_count=20)`\n",
    "\n",
    "Both approaches are available - use the new approach for more statistically accurate personas!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d37ef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_personas_with_stats_sync(personas_count=10, model=MODEL_TO_USE):\n",
    "    \"\"\"\n",
    "    Synchronously generate personas with predefined statistics (for testing).\n",
    "    \n",
    "    Args:\n",
    "        personas_count (int): Number of personas to generate\n",
    "        model (str): Model to use\n",
    "    \n",
    "    Returns:\n",
    "        list: List of completed persona dictionaries\n",
    "    \"\"\"\n",
    "    # Generate base personas\n",
    "    base_personas = [generate_base_persona_complete() for _ in range(personas_count)]\n",
    "    \n",
    "    # Create messages\n",
    "    msgs = messages_with_base(base_personas)\n",
    "    \n",
    "    # Call API\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=msgs,\n",
    "        temperature=TEMPERATURE,\n",
    "        top_p=TOP_P,\n",
    "        presence_penalty=PRESENCE_PENALTY,\n",
    "        frequency_penalty=FREQUENCY_PENALTY\n",
    "    )\n",
    "    \n",
    "    # Parse response\n",
    "    content = response.choices[0].message.content\n",
    "    personas = json.loads(content)\n",
    "    \n",
    "    return personas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc4e5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test synchronous generation (uncomment to run)\n",
    "# print(\"Generating 2 personas with predefined statistics...\")\n",
    "# completed_personas = generate_personas_with_stats_sync(personas_count=2)\n",
    "# \n",
    "# print(f\"\\nGenerated {len(completed_personas)} complete personas:\")\n",
    "# for i, persona in enumerate(completed_personas, 1):\n",
    "#     print(f\"\\n{'='*80}\")\n",
    "#     print(f\"Persona {i}:\")\n",
    "#     print('='*80)\n",
    "#     print(json.dumps(persona, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c789e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_base_persona():\n",
    "    # This function will generate a base persona using available data\n",
    "\n",
    "    #Gendeer\n",
    "    # F: 53%\n",
    "    # M: 47%\n",
    "\n",
    "    # Age\n",
    "    # Should follow decaing distribution of age\n",
    "\n",
    "    # Code:\n",
    "    gender = random.choices([\"Female\", \"Male\"], weights=[53, 47])[0]\n",
    "    age = random.choices(range(65, 95), weights=[10, 15, 20, 25, 15, 10, 5, 3, 2, 1])[0]\n",
    "    marital_status = random.choices([\"Married\", \"Single\", \"Divorced\", \"Widowed\"], weights=[60, 30, 5, 5])[0]\n",
    "    children = random.choices([\"0\", \"1\", \"2\", \"3+\"], weights=[5, 15, 30, 50])[0]\n",
    "    living_situation = random.choices([\"Living with Family\", \"Living Alone\", \"Living with Partner\"], weights=[50, 30, 20])[0]\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73339227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You must generate a set of fictional but realistic Iranian elderly personas that represent cultural, geographical, and social diversity in Iran.\n",
      "\n",
      "Rules:\n",
      "- All personas must be elderly (age 65–90).\n",
      "- Diversity must be reflected across all components, but keep the integrity and realism of each persona.\n",
      "- Personas should reflect cultural and social realities of Iran.\n",
      "- Reactions and attitudes do not need to be \"correct\" or \"moral\"; they may be shaped by culture, personal experience, or limitations.\n",
      "- Personality traits and psychological states should be consistent with the individual’s background.\n",
      "- Use the JSON format provided below.\n",
      "- For every variable, use values consistent with Iranian context.\n",
      "- Do not omit any variable.\n",
      "\n",
      "Variable definitions and accepted values:\n",
      "Here’s the translation of your variables and values into **English**:\n",
      "\n",
      "### 1. **Demographic Information**\n",
      "\n",
      "* **Age**: integer\n",
      "* **Gender**: [\"Male\", \"Female\"]\n",
      "* **Marital Status**: [\"Single\", \"Married\", \"Widowed\", \"Divorced\"]\n",
      "* **Children**: number or description (e.g., \"None\", \"1\", \"2-3\", \"4+\")\n",
      "* **Living Situation**: [\"Living with Family\", \"Living Alone\", \"Shared Housing\"]\n",
      "\n",
      "### 2. **Biological Component**\n",
      "\n",
      "* **General Health**: [\"Good\", \"Average\", \"Poor\"]\n",
      "* **Chronic Disease**: [None, \"High Blood Pressure\", \"Cardiovascular Diseases\", \"Type 2 Diabetes\", \"Arthritis and Joint Pain\", \"Osteoporosis\", \"Alzheimer's and Dementia\", \"Chronic Kidney Disease\", \"Chronic Obstructive Pulmonary Disease\", \"Chronic Depression and Anxiety\", \"Vision and Hearing Problems\", \"Chronic Liver Failure\", \"Parkinson's\", \"Chronic Sleep Disorders\", \"Chronic Gastrointestinal Issues\"]\n",
      "* **Mobility**: [\"Independent\", \"With Cane or Walker\", \"In Wheelchair\", \"Dependent\"]\n",
      "* **Hearing Senses**: [\"Good\", \"Average\", \"Poor\"]\n",
      "* **Vision Senses**: [\"Good\", \"Average\", \"Poor\"]\n",
      "* **Daily Energy**: [\"High\", \"Average\", \"Low\"]\n",
      "\n",
      "### 3. **Psychological Component**\n",
      "\n",
      "* **Personality Type**: [\"INTJ\", \"INTP\", \"ENTJ\", \"ENTP\", \"INFJ\", \"INFP\", \"ENFJ\", \"ENFP\", \"ISTJ\", \"ISFJ\", \"ESTJ\", \"ESFJ\", \"ISTP\", \"ISFP\", \"ESTP\", \"ESFP\"]\n",
      "* **Cognitive Status**: [\"Healthy Memory\", \"Mild Forgetfulness\", \"Alzheimer's\"]\n",
      "* **Dominant Emotion**: [\"Happy\", \"Sad\", \"Anxious\", \"Calm\"]\n",
      "* **Emotional Intelligence**: [\"Low\", \"Average\", \"High\"]\n",
      "* **IQ**: [\"Low\", \"Average\", \"High\"]\n",
      "* **Attitude Toward Aging**: [\"Acceptance\", \"Resistance\", \"Meaning-Seeking\", \"Denial\"]\n",
      "\n",
      "### 4. **Social Component**\n",
      "\n",
      "* **Main Social Role**: [\"Grandfather\", \"Grandmother\", \"Retired\", \"Social Activist\"]\n",
      "* **Social Support**: [\"Large Family\", \"Alone\", \"Supportive Friends\", \"Government Support\"]\n",
      "* **Social Participation**: [\"Active\", \"Inactive\"]\n",
      "\n",
      "### 5. **Economic Component**\n",
      "\n",
      "* **Income**: [\"Independent\", \"Retirement Pension\", \"Dependent on Children\", \"No Income\"]\n",
      "* **Economic Decile**: integer 1–10\n",
      "* **Housing**: [\"Own Home\", \"Rented\", \"Nursing Home\"]\n",
      "\n",
      "### 6. **Cultural-Value Component**\n",
      "\n",
      "* **Religion & Sect**: [\"Shia Muslim\", \"Sunni Muslim\", \"Christian\", \"Zoroastrian\", \"Jewish\"]\n",
      "* **Internalized Moral Traits**: list of 2–4 traits (positive or negative)\n",
      "* **Religiosity Level**: [\"Low\", \"Average\", \"High\"]\n",
      "* **Ethnicity**: [\"Persian\", \"Azeri\", \"Kurdish\", \"Lur\", \"Baloch\", \"Arab\", \"Turkmen\", \"Gilaki\", \"Mazandarani\", \"Qashqai\"]\n",
      "* **Language**: [\"Persian\", \"Azeri\", \"Kurdish\", \"Luri\", \"Balochi\", \"Arabic\", \"Turkmen\", \"Gilaki\", \"Mazandarani\", \"Qashqai\"]\n",
      "\n",
      "### 7. **Contextual Component**\n",
      "\n",
      "* **Important Personal Experiences**: [\"Immigration\", \"Career Success\", \"Loss of Loved Ones\", \"War Experience\", \"Economic Hardship\", \"Educational Achievement\", \"Battle with Serious Illness (e.g., Cancer, Chronic Disease)\"]\n",
      "* **Life Satisfaction**: [\"Satisfied\", \"Dissatisfied\", \"Neutral\"]\n",
      "* **Meaning and Purpose in Old Age**: [\"Helping Family\", \"Spiritual Activities\", \"Waiting for Death\", \"Pleasure-Seeking\"]\n",
      "\n",
      "\n",
      "JSON Format:\n",
      "[\n",
      "{\n",
      "  \"age\": \"\",\n",
      "  \"gender\": \"\",\n",
      "  \"marital_status\": \"\",\n",
      "  \"children\": \"\",\n",
      "  \"living_situation\": \"\",\n",
      "  \"general_health\": \"\",\n",
      "  \"chronic_disease\": \"\",\n",
      "  \"mobility\": \"\",\n",
      "  \"hearing_senses\": \"\",\n",
      "  \"vision_senses\": \"\",\n",
      "  \"daily_energy\": \"\",\n",
      "  \"personality_type\": \"\",\n",
      "  \"cognitive_status\": \"\",\n",
      "  \"dominant_emotion\": \"\",\n",
      "  \"emotional_intelligence\": \"\",\n",
      "  \"iq\": \"\",\n",
      "  \"attitude_toward_aging\": \"\",\n",
      "  \"main_social_role\": \"\",\n",
      "  \"social_support\": \"\",\n",
      "  \"social_participation\": \"\",\n",
      "  \"income\": \"\",\n",
      "  \"economic_decile\": \"\",\n",
      "  \"housing\": \"\",\n",
      "  \"religion_and_sect\": \"\",\n",
      "  \"internalized_moral_traits\": \"\",\n",
      "  \"religiosity_level\": \"\",\n",
      "  \"ethnicity\": \"\",\n",
      "  \"language\": \"\",\n",
      "  \"important_personal_experiences\": \"\",\n",
      "  \"life_satisfaction\": \"\",\n",
      "  \"meaning_and_purpose_in_old_age\": \"\"\n",
      "},\n",
      "...\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39b57b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "BASE_URL = \"https://api.metisai.ir/openai/v1\"\n",
    "API_KEY = os.getenv(\"METIS_API_KEY\")\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=API_KEY,\n",
    "    base_url=BASE_URL,\n",
    "    http_client=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04619e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TO_USE = \"gpt-5-mini\"\n",
    "\n",
    "TEMPERATURE = 1\n",
    "TOP_P = 0.9\n",
    "PRESENCE_PENALTY = 0.3\n",
    "FREQUENCY_PENALTY = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "946f3b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages(persona_count: int):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Generate {persona_count} personas. Only give the JSON array with no extra text or formatting. Don't wrap the array in markdown formatting.\"\n",
    "        }\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef683686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm_batch(model, messages, batch_count=1, batch_file_path=\"batch_input.jsonl\"):\n",
    "    \"\"\"\n",
    "    Call the OpenAI Batch API to process requests asynchronously.\n",
    "\n",
    "    Args:\n",
    "        model (str): The model to use.\n",
    "        messages (list): The list of messages to send (single prompt with system and user roles).\n",
    "        batch_count (int): The number of batches to create.\n",
    "        batch_file_path (str): Path to save the batch input file.\n",
    "\n",
    "    Returns:\n",
    "        dict: Batch metadata including status and file IDs.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import os\n",
    "\n",
    "    # Step 1: Prepare the batch input file\n",
    "    with open(batch_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i in range(batch_count):\n",
    "            batch_request = {\n",
    "                \"custom_id\": f\"request-{i+1}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": model,\n",
    "                    \"messages\": messages,\n",
    "                }\n",
    "            }\n",
    "            f.write(json.dumps(batch_request, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # Step 2: Upload the batch input file\n",
    "    batch_input_file = client.files.create(\n",
    "        file=open(batch_file_path, \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "\n",
    "    # Step 3: Create the batch\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=batch_input_file.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\n",
    "            \"description\": \"Batch processing for persona generation\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48a9912d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1310\n",
      "{'single_persona_tokens': 278, 'total_personas_tokens': 13900}\n"
     ]
    }
   ],
   "source": [
    "# Token counting helpers using tiktoken\n",
    "# Requires: pip install tiktoken\n",
    "import tiktoken\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "def num_tokens_from_messages(messages: List[Dict[str, Any]], model: str = MODEL_TO_USE) -> int:\n",
    "    \"\"\"Return an estimate of the number of tokens used by a list of chat messages.\n",
    "\n",
    "    Uses heuristics commonly used with OpenAI chat models (tokens per message/name),\n",
    "    falling back to the `cl100k_base` encoding when the model encoding isn't available.\n",
    "\n",
    "    Note: exact token accounting depends on the model's internal tokenization; this\n",
    "    function gives a good practical estimate for budgeting and monitoring.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except Exception:\n",
    "        # fallback if model name is unknown to tiktoken\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "    # Heuristics from public guidance; adjust if you know exact model rules\n",
    "    if model in (\"gpt-3.5-turbo-0301\", \"gpt-4-0314\"):\n",
    "        tokens_per_message = 4\n",
    "        tokens_per_name = -1\n",
    "    else:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "\n",
    "    total_tokens = 0\n",
    "    for message in messages:\n",
    "        total_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            # skip non-string values by converting to string\n",
    "            if not isinstance(value, str):\n",
    "                value = json.dumps(value, ensure_ascii=False)\n",
    "            total_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                total_tokens += tokens_per_name\n",
    "\n",
    "    total_tokens += 3  # assistant priming (heuristic)\n",
    "    return total_tokens\n",
    "\n",
    "\n",
    "def num_tokens_from_string(s: str, model: str = MODEL_TO_USE) -> int:\n",
    "    \"\"\"Return the number of tokens in a string for the given model's tokenizer.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except Exception:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(encoding.encode(s))\n",
    "\n",
    "\n",
    "SAMPLE_PERSONA = {\n",
    "    \"age\": 65,\n",
    "    \"gender\": \"Male\",\n",
    "    \"marital_status\": \"Married\",\n",
    "    \"children\": \"2-3\",\n",
    "    \"living_situation\": \"Living with Family\",\n",
    "    \"general_health\": \"Good\",\n",
    "    \"chronic_disease\": \"High Blood Pressure\",\n",
    "    \"mobility\": \"Independent\",\n",
    "    \"hearing_senses\": \"Good\",\n",
    "    \"vision_senses\": \"Good\",\n",
    "    \"daily_energy\": \"High\",\n",
    "    \"personality_type\": \"ISTJ\",\n",
    "    \"cognitive_status\": \"Healthy Memory\",\n",
    "    \"dominant_emotion\": \"Calm\",\n",
    "    \"emotional_intelligence\": \"High\",\n",
    "    \"iq\": \"Average\",\n",
    "    \"attitude_toward_aging\": \"Acceptance\",\n",
    "    \"main_social_role\": \"Grandfather\",\n",
    "    \"social_support\": \"Large Family\",\n",
    "    \"social_participation\": \"Active\",\n",
    "    \"income\": \"Retirement Pension\",\n",
    "    \"economic_decile\": 6,\n",
    "    \"housing\": \"Own Home\",\n",
    "    \"religion_and_sect\": \"Shia Muslim\",\n",
    "    \"internalized_moral_traits\": [\"Respectful\", \"Reliable\", \"Generous\"],\n",
    "    \"religiosity_level\": \"Average\",\n",
    "    \"ethnicity\": \"Persian\",\n",
    "    \"language\": \"Persian\",\n",
    "    \"important_personal_experiences\": \"Educational Achievement\",\n",
    "    \"life_satisfaction\": \"Satisfied\",\n",
    "    \"meaning_and_purpose_in_old_age\": \"Helping Family\"\n",
    "}\n",
    "\n",
    "\n",
    "def estimate_persona_tokens(persona_sample: Dict[str, Any], persona_count: int, model: str = MODEL_TO_USE) -> Dict[str, int]:\n",
    "    \"\"\"Estimate tokens for a single persona JSON and for persona_count copies of it.\n",
    "\n",
    "    Returns a dict with single_persona_tokens and total_personas_tokens.\n",
    "    \"\"\"\n",
    "    persona_str = json.dumps(persona_sample, ensure_ascii=False)\n",
    "    single = num_tokens_from_string(persona_str, model)\n",
    "    return {\"single_persona_tokens\": single, \"total_personas_tokens\": single * persona_count}\n",
    "\n",
    "\n",
    "def estimate_run_tokens(persona_count: int, response_text: str, model: str = MODEL_TO_USE) -> Dict[str, int]:\n",
    "    \"\"\"Estimate input/output tokens for a run that asks for persona_count personas.\n",
    "\n",
    "    - input_tokens: tokens consumed by the `messages(persona_count)` payload\n",
    "    - output_tokens: tokens in the LLM response text\n",
    "    - total: sum of input + output\n",
    "    \"\"\"\n",
    "    msgs = messages(persona_count)\n",
    "    input_tokens = num_tokens_from_messages(msgs, model)\n",
    "    output_tokens = num_tokens_from_string(response_text or \"\", model)\n",
    "    return {\"input_tokens\": input_tokens, \"output_tokens\": output_tokens, \"total\": input_tokens + output_tokens}\n",
    "\n",
    "\n",
    "# Usage examples (uncomment to run):\n",
    "print(num_tokens_from_messages(messages(1)))\n",
    "print(estimate_persona_tokens(SAMPLE_PERSONA, 50))\n",
    "# if 'personas' in globals():\n",
    "#     print(estimate_run_tokens(50, personas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7dfc7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types import Batch\n",
    "\n",
    "def poll_batch_status(batch: Batch):\n",
    "    batch_id = batch.id\n",
    "\n",
    "    resp = client.batches.retrieve(batch_id)\n",
    "\n",
    "    if resp.status == \"completed\":\n",
    "        if resp.output_file_id:\n",
    "            file_response = client.files.content(resp.output_file_id)\n",
    "            return file_response\n",
    "        elif resp.error_file_id:\n",
    "            file_response = client.files.content(resp.error_file_id)\n",
    "            return file_response\n",
    "        else:\n",
    "            print(\"Batch completed but no output_file_id\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Batch status: {resp.status}. Not completed yet.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "634fc914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(resp):\n",
    "    contents = []\n",
    "    if resp:\n",
    "        answers = resp.text.split(\"\\n\")[:-1]\n",
    "\n",
    "        for batch_answer in answers:\n",
    "            single_batch_resp = json.loads(batch_answer)\n",
    "\n",
    "            answer = single_batch_resp['response']['body']['choices'][0]['message']['content']\n",
    "            contents.append(answer)\n",
    "\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b07b3b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def save_batch_output(batch):\n",
    "    timestamp = time()\n",
    "\n",
    "    output_path = f\"personas/batch_output_{timestamp}.jsonl\"\n",
    "    result = poll_batch_status(batch)\n",
    "    if not result:\n",
    "        print(\"Batch not completed yet or no output available.\")\n",
    "        return\n",
    "        \n",
    "    parsed = parse_response(result)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for p in parsed:\n",
    "            f.write(json.dumps(json.loads(p)) + \"\\n\")\n",
    "\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "879390a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the capital of France?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "resp = call_llm_batch(MODEL_TO_USE, test_messages, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84787aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The capital of France is Paris.', 'The capital of France is Paris.']\n"
     ]
    }
   ],
   "source": [
    "result = poll_batch_status(resp)\n",
    "if result:\n",
    "    print(parse_response(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d4dc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = call_llm_batch(MODEL_TO_USE, messages(10), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18619725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openai._legacy_response.HttpxBinaryResponseContent at 0x7358f65638c0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = poll_batch_status(resp)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9543521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_batch_output(resp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
